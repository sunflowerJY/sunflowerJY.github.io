<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[校招编程题总结]]></title>
    <url>%2F2018%2F09%2F06%2F%E6%A0%A1%E6%8B%9B%E7%BC%96%E7%A8%8B%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[字节跳动 2018.8.25 19校招第二次笔试 字节跳动大闯关 题目描述： Bytedance Efficiency Engineering团队在8月20日搬入了学清嘉创大厦。为庆祝团队的乔迁之喜，字节君决定邀请整个EE团队，举办一个大型团建游戏-字节跳动大闯关。可是遇到了一个问题： EE团队共有n个人，大家都比较害羞，不善于与陌生人交流。这n个人每个人都向字节君提供了自己认识人的名字，不包括自己。如果A的名单里有B，或B的名单里有A，则代表A与B相互认识。同时如果A认识B，B认识C，则代表A与C也会很快认识，毕竟通过B的介绍，两个人就可以很快相互认识的了。 为了大闯关游戏可以更好地团队协作、气氛更活跃，并使得团队中的人可以尽快的相互了解、认识和交流，字节君决定根据这个名单将团队分为m组，每组人数可以不同，但组内的任何一个人都与组内的其他所有人直接或间接的认识和交流。如何确定一个方案，使得团队可以分成m组，并且这个m尽可能地小呢？ Key:并查集 注意，题目意思是第一行10，代表一共有10个人，第二行0，代表编号为1的人不认识任何人，第三行0 5 3，代表编号为2的人认识编号5和编号3， 合法的表达式 双生词 空气质量 减法求值]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习]]></title>
    <url>%2F2018%2F09%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[算法分类 学习方法 监督式学习 非监督式学习 半监督式学习 强化学习 lazy/eager lazy方法 如果算法对整个训练数据集并没有训练得到一个整体的模型，对于每一个新的测试数据点，都需要根据该点和训练数据集来对目标函数进行预测，这叫做lazy method。 eager方法 早早的根据训练数据集把模型建好，对与新的测试数据，只需要往模型中代入就可以得到结果了。先算好模型再进行预测的算法叫做eager method。 lazy method 典型算法是KNN、LWR(locally weighted regression)、Case-based reasoning。 eager method 除了lazy method中提到的三种算法之外，几乎所有的机器学习算法都可以认为是eager method，比如线性回归、逻辑回归、ANN、SVM、决策树、relation rule, etc., 因为他们都是根据训练数据集建立好模型从而对新数据进行预测的。 算法类似性 决策树学习 根据数据的属性采用树状结构建立决策模型。决策树模型常常用来解决分类和回归问题。常见的算法包括 CART (Classification And Regression Tree)、ID3、C4.5、随机森林 (Random Forest) 等。 回归算法 试图采用对误差的衡量来探索变量之间的关系的一类算法。常见的回归算法包括最小二乘法 (Least Square)、逻辑回归 (Logistic Regression)、逐步式回归 (Stepwise Regression) 等。 聚类算法 通常按照中心点或者分层的方式对输入数据进行归并。所有的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 K-Means 算法以及期望最大化算法 (Expectation Maximization) 等。 人工神经网络 模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络算法包括感知器神经网络 (Perceptron Neural Network) 、反向传递 (Back Propagation) 和深度学习等。 集成算法 用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括 Boosting、Bagging、AdaBoost、随机森林 (Random Forest) 等。 决策树 参考：https://www.jianshu.com/p/fb97b21aeb1d 决策树是附加概率结果的一个树状的决策图，是直观的运用统计概率分析的图法。机器学习中决策树是一个预测模型，它表示对象属性和对象值之间的一种映射，树中的每一个节点表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。 预测是否具有偿还贷款的能力： 基本知识 信息增益 信息熵表示的是不确定度。均匀分布时，不确定度最大，此时熵就最大。当选择某个特征对数据集进行分类时，分类后的数据集信息熵会比分类前的小，其差值表示为信息增益。信息增益可以衡量某个特征对分类结果的影响大小。 基尼系数 基尼指数反应了从数据集中随机抽取两个样本，其类标不一致的概率。 决策树算法 决策树的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化。 所以决策树的本质和其他机器学习模型是一致的，有一个损失函数，然后去优化这个函数；然而，区别就在于如何优化。 决策树采用启发式算法来近似求解最优化问题，得到的是次最优的结果。 该启发式算法可分为三步： 特征选择 模型生成 决策树的剪枝 决策树学习算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割。 选择最优特征要根据特征的分类能力，特征分类能力的衡量通常采用信息增益或信息增益比。 决策树学习常用的算法主要有以下三种： ID3算法，C4.5算法，CART算法。 算法总结: ID3算法/C4.5算法/CART算法。 ID3算法和C4.5算法用于生成分类树，区别主要在于选取特征的依据，前者是信息增益，后者是信息增益比。 CART算法可以生成分类树和回归树，分类树使用基尼指数选取特征，并且不用提前确定α值，而是在剪枝的同时找到最优的α值. 集成学习 Bagging bagging:bootstrap aggregating 的缩写。 bagging是一种并行式集成学习方法，可用于二分类，多分类，回归等任务。 基本流程 对一个包含 m 个样本的数据集，有放回地进行 m 次随机采样，这样得到具有 m 个样本的采样集。 取 T 个这样的采样集。 每个采样集训练一个基学习器。 结合：分类任务，使用简单投票法。回归任务，使用简单平均法。 有放回抽样的好处? 这种有放回抽样会有 63.2% 的样本出现在采样集中，而剩下的 36.8% 样本可以作为验证集对模型的泛化性能进行包外估计。 当基学习器是决策树时，可以用包外样本来辅助剪枝， 还可以用于估计决策树中各结点的后验概率来辅助对零训练样本结点的处理。 基学习器是神经网络时，用包外样本来辅助早期停止来减小过拟合。 Bagging特点 Bagging 主要关注降低方差，是要降低过拟合，而不会降低偏差，因此最好不要用高偏差的模型。 在不剪枝决策树，神经网络等易受样本扰动的学习器上效用更为明显。 例如当基学习器是决策树时，Bagging 是并行的生成多个决策树，此时可以不做剪枝，这样每个都是强学习器，就会有过拟合的问题，但是多个学习器组合在一起，可以降低过拟合。 随机森林 随机森林和Bagging同属于弱依赖学习器，前面已经介绍了Bagging，随机森林是进阶版Bagging。 随机森林的思想仍然是Bagging，它使用CART决策树作为弱学习器，而RF进阶在于对决策树的建立做了改进。对于普通的决策树，在节点上所有的n个样本特征中选择最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的部分样本特征$n_{sub}$，再在这些随机选择的$n_{sub}$个样本特征中选择最优的特征做决策树的左右子树划分，这样做进一步加强了模型的泛化能力。 针对结合策略，如果是分类问题，则采用简单投票法；如果是回归问题，则采用算术平均法。 Bagging 和 Boosting 的区别 样本选择： Bagging 的训练集是在原始集中有放回选取的，各轮训练集之间是独立的，每个样例的权重相等； Boosting 的训练集不变，只是每个样例在分类器中的权重发生变化，错误的样本会得到更大的重视； Bagging 的预测函数没有权重之分；Boosting 的预测函数是有权重之分，效果好的函数权重大； Bagging 的各个预测函数并行产生，容易 map-reduce ，Boosting 的预测是顺序产生，后一个模型参数需要前一轮模型的结果。]]></content>
  </entry>
  <entry>
    <title><![CDATA[校招考点大全]]></title>
    <url>%2F2018%2F08%2F29%2FHadoop%E5%9F%BA%E7%A1%80%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[算法 排序 大数据 Hive 简介 Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载(ETL)，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。 Hive构建在Hadoop的HDFS和MapReduce之上，用于管理和查询结构化/非结构化数据的数据仓库。 使用HQL作为查询接口 使用HDFS作为底层存储 使用MapReduce作为执行计算 HQL查询语句 Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。 同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 基本操作 CREATE TABLE 创建一个指定名字的表。 EXTERNAL 关键字可以让用户创建一个外部表。 在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 LIKE 允许用户复制现有的表结构，但是不复制数据。 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。 如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe(Serialize/Deserilize，序列化和反序列化)。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE 。 有分区的表可以在创建的时候使用 PARTITIONED BY 语句。 一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下。而且，表和分区都可以对某个列进行 CLUSTERED BY 操作，将若干个列放入一个桶（bucket）中。也可以利用SORT BY 对数据进行排序。这样可以为特定应用提高性能。 表名和列名不区分大小写，SerDe 和属性名区分大小写。表和列的注释是字符串 MapReduce 在hive的查询中，大部分都会出发一个MapReduce操作，但是在hive中，某些情况可以不必使用MapReduce，也就是所谓的本地操作模式。 select * from tableName Hive可以简单的读取table对应的存储目录下的文件，然后输出格式化后的文件到控制台。 对于在where条件中使用分区字段这种情况，也是无需MapReduce过程的，例如select * from tableName where partition1=‘’；无论使用limit语句限制记录条数。 hive.exec.mode.local.auto = true 元数据存储 Hive中metastore（元数据存储）的三种方式： 内嵌Derby方式 Derby作为一个内嵌的元数据库，可以完成hive安装的简单测试。 当在某个目录下启动终端，进入hive shell时，hive默认会在当前目录下生成一个derby文件和一个metastore_db目录，这两个文件主要保存刚刚在shell中操作的一些sql的结果，比如新建的表、添加的分区等等。 弊端： 在同一个目录下同时只能有一个hive客户端能使用数据库 切换目录启动新的shell，无法查看之前创建的表，不能实现表数据的共享 Local方式 由于使用默认的元数据库有些弊端，所以采用本地mysql保存hive元数据解决上面的问题。hive所有的元数据都保存在同一个库里，这样不同开发者创建的表可以实现共享。 Remote方式 使用远端mysql服务器存储元数据。这种存储方式需要在远端服务器运行一个mysql服务器，并且需要在Hive服务器启动meta服务。 Hadoop 文件存放策略 Hadoop在设计时考虑到数据的安全与高效，数据文件默认在HDFS上存放三份。 存储策略为本地一份，同机架内其它某一节点上一份，不同机架的某一节点上一份。 这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿数据要快；同时，如果整个机架的网络出现异常，也能保证在其它机架的节点上找到数据。 为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。那么Hadoop是如何确定任意两个节点是位于同一机架，还是跨机架的呢？ 答案就是机架感知。 机架感知配置 高可用架构 参考https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html HDFS中NameNode等的HA是基于ZooKeeper实现的。 Hadoop HDFS NameNode HA的设计图： NameNode（NN）、DataNode（DN）均是HDFS中的组件或节点，NN有两个，分为Active和Standby状态，DN有多个，DN周期性向Active和Standby做数据块汇报； 其次，处于中间位置的，实现HDFS NameNode HA最关键的组件是FailoverController，它是运行在NameNode节点上的守护进程，将Active NN节点的信息通过竞争的方式注册到ZooKeeper上，并实现以下三个基本功能： 有一个后台线程，周期性的检查NameNode的健康状况，即粗的红色拐弯箭头的Monitor Health，触发状态选举等； 定期保持与ZooKeeper的心跳，完成Leader选举（即Active节点选举）和检查Active NN ZNode节点状态等工作，即最上面与ZooKeeper交互的蓝色箭头； 向NameNode发送一些命令，完成状态切换。 NN HA的主体思路: NN竞争在ZooKeeper上注册，即create一个临时节点，写入NN的host、port、nameserviceId、namenodeI等信息，哪个写入成功，哪个就是Active状态； 注册成功后，通过create后的watcher机制，FailoverController会发送命令给各个NN，让其确定各自状态和职责； Monitor Health会周期性连接NN，检查NN状态，并由可能触发重新选举，即重复1-2； FailoverController会与ZooKeeper保持心跳，注册的临时节点消失后，也会触发重新选举。 Spark 参考：https://www.cnblogs.com/ITtangtang/p/7967386.html 简介 Spark是类似Hadoop MapReduce的通用的并行计算框架。 Spark基于map reduce算法实现的分布式计算，拥有HadoopMapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的map reduce的算法。 Spark架构： 基本概念 RDD 弹性分布式数据集。 Operation 作用于RDD的各种操作分为transformation和action。 Job 一个作业包含多个RDD及作用于相应RDD上的各种operation。 包含多个Task组成的并行计算，往往由Spark Action触发生成， 一个Application中往往会产生多个Job。 Stage 一个作业分为多个阶段。 每个Job会被拆分成多组Task，作为一个TaskSet，其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方。 Task 被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责。 Partition 一个RDD中的数据可以分成多个不同的区。 DAG Directed Acycle graph，有向无环图，反应RDD之间的依赖关系。 DAGScheduler 实现将Spark作业分解成一到多个Stage。 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图)，并提交Stage给TASkScheduler。 其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法。 TaskScheduler 将TaskSET提交给worker运行，每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet，当Executor向Driver发生心跳时，TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行标签，重试失败的Task。 在不同运行模式中任务调度器具体为： Spark on Standalone模式为TaskScheduler YARN-Client模式为YarnClientClusterScheduler YARN-Cluster模式为YarnClusterScheduler Application 指用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码 Driver Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用SparkContext代表Driver Executor 某个Application运行在worker节点上的一个进程，该进程负责运行某些Task，并且负责将数据存到内存或磁盘上，每个Application都有各自独立的一批Executor，在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象，负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task，这个每一个CoarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数 Cluter Manager 在集群上获取资源的外部服务，共三种类型： Standalone:spark原生的资源管理，由Master负责资源的分配 Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架 Hadoop Yarn: 主要是指Yarn中的ResourceManager Worker 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NoteManager节点。 Narrow dependency 窄依赖，子RDD依赖于父RDD中固定的data partition。 Wide Dependency 宽依赖，子RDD对父RDD中的所有data partition都有依赖。 Caching Managenment 缓存管理，对RDD的中间计算结果进行缓存管理以加快整 体的处理速度。 一个任务从分发到执行的流程框架： Akka Spark模块间(Client,Master,Worker)通信是基于Scala原生支持的akka。 RDD操作类型 Spark提供的数据集操作类型有很多种，主要分为2类。 transformations 如map, filter, flatMap, sample, groupByKey, reduceByKey, union, join,cogroup, mapValues, sort,partionBy等多种操作类型。 actions 如Count, collect, reduce, lookup, save等多种操作类型。 无论执行了多少次transformation操作，RDD都不会真正执行运算，只有当action操作被执行时，运算才会触发。 运行JOB两种模式 cluster模式 Spark driver在 application的master process中运行。如果和YARN集成，则application master process由YARN管理，在YARN中运行。 cluster mode without yarn 客户端提交作业给Master Master让一个Worker启动Driver,即SchedulerBackend。 Worker创建一个DriverRunner线程，DriverRunner启动SchedulerBackend进程。 client模式 Spark driver在clinet process中运行。如果集成YARN，application master只负责从YARN请求资源。 分布式部署方式 支持四种分布式部署方式。 local模式 local 测试或实验性质的本地运行模式(单机) 用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。 在程序执行过程中，只会生成一个SparkSubmit进程。 这个SparkSubmit进程又当爹、又当妈，既是客户提交任务的Client进程、又是Spark的driver程序、还充当着Spark执行Task的Executor角色。 local cluster 测试或实验性质的本地伪集群运行模式(单机模拟集群) 在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委屈求全的共享资源。 通常也是用来验证开发出来的应用程序逻辑上有没有问题，或者想使用Spark的计算框架而没有太多资源。 SparkSubmit依然充当全能角色，又是Client进程，又是driver程序，还有点资源管理的作用。生成的两个CoarseGrainedExecutorBackend，就是用来并发执行程序的进程。 standalone模式 独立模式。该模式是其他两种模式的基础。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的。 在执行应用程序前，必须先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容。 standalone client模式 提交应用程序： 所有有Worker进程的节点上启动Executor来执行应用程序。 Master进程做为cluster manager，用来对应用程序申请的资源进行管理； SparkSubmit 做为Client端和运行driver程序； CoarseGrainedExecutorBackend 用来并发执行应用程序； standalone cluster模式 客户端的SparkSubmit进程会在应用程序提交给集群之后就退出 Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序,而该DriverWrapper 进程会占用Worker进程的一个core Spark On Mesos模式 Spark On YARN模式 基于YARN的Resource Manager的Client模式 Spark跑在Hadoop集群中,要使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。 在执行Spark应用程序前，要启动Hadoop的各种服务。由于已经有了资源管理器，所以不需要启动Spark的Master、Worker守护进程。 提交应用程序后： 在Resource Manager(RM)节点上提交应用程序，会生成SparkSubmit进程，该进程会执行driver程序。 RM会在集群中的某个NodeManager上，启动一个ExecutorLauncher进程，来做为ApplicationMaster。另外，也会在多个NodeManager上生成CoarseGrainedExecutorBackend进程来并发的执行应用程序。 基于YARN的Resource Manager的Custer模式 在Resource Manager端提交应用程序，会生成SparkSubmit进程，该进程只用来做Client端，应用程序提交给集群后，就会删除该进程。 Resource Manager在集群中的某个NodeManager上运行ApplicationMaster(AM)，该AM同时会执行driver程序。紧接着，会在各NodeManager上运行CoarseGrainedExecutorBackend来并发执行应用程序。 Standalone模式 Standalone模式下的角色 Client：客户端进程，负责提交作业到Master。 Master：Standalone模式中主控节点，负责接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。 Worker：Standalone模式中slave节点上的守护进程，负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，启动Driver和Executor。 Driver： 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGScheduler，TaskScheduler。 Executor：即真正执行作业的地方，一个集群一般包含多个Executor，每个Executor接收Driver的命令Launch Task，一个Executor可以执行一到多个Task。 Standalone模式流程 Driver 的启动： SparkSubmit提交程序后启动Client进程， 创建Client.ClientEndpoint ,onStart() 内向 Master 发送 RequestSubmitDriver 消息Master 接收请求调用schedule()方法中发出launchDriver消息，由worker启动 Client 的启动： SparkSubmit提交程序=&gt;org.apache.spark.deploy.Client.main() Executor 的启动： new SparkContext() =&gt; taskScheduler.start =&gt; backend.start() =&gt; backend.start()内client=new AppClient() =&gt; client.start() =&gt; ClientEndpoint.onStart()内向 Master发送RegisterApplication 请求 =&gt; Master 授理请求，执行 startExecutorsOnWorkers() ，启动各个 Executor , 该方法含资源分配]]></content>
  </entry>
  <entry>
    <title><![CDATA[”魔镜杯“风控算法大赛]]></title>
    <url>%2F2018%2F08%2F14%2F%E2%80%9D%E9%AD%94%E9%95%9C%E6%9D%AF%E2%80%9C%E9%A3%8E%E6%8E%A7%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B%2F</url>
    <content type="text"><![CDATA[介绍 拍拍贷是一家互联网金融公司，在2016年拍拍贷举办“魔镜杯”风险算法大赛，首度公开真实的历史数据，旨在寻求高效准确的预测算法， 为公司投资人提供决策依据，促进健康高效的互联网金融。 数据 为保护用户隐私安全，项目所用数据均已经经过脱敏处理。 数据主要分为3个表：Master、Log_Info、Userupdate_Info 每一行代表一个样本(一笔成功成交借款),每个样本包含200多个各类字段。 1.Master 主要字段 描述 idx 每一笔贷款的unique key，可以与另外2个文件里的idx相匹配 UserInfo 借款人特征字段 WeblogInfo Info网络行为字段 Education 学历学籍字段 ThirdParty_Info_PeriodN 第三方数据时间段N字段 SocialNetwork 社交网络字段 LinstingInfo 借款成交时间 Target 违约标签（1/0,违约/正常）测试集不包含target字段 1234567train_master.info()》&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 30000 entries, 0 to 29999Columns: 228 entries, Idx to ListingInfodtypes: float64(38), int64(170), object(20)memory usage: 52.2+ MB 2.Log_Info 借款人的登陆信息。 主要字段 描述 ListingInfo 借款成交时间 LogInfo1 操作代码 LogInfo2 操作类别 LogInfo3 登陆时间 idx 每一笔贷款的unique key 123456789101112train_loginfo.info()》&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 580551 entries, 0 to 580550Data columns (total 5 columns):Idx 580551 non-null int64Listinginfo1 580551 non-null objectLogInfo1 580551 non-null int64LogInfo2 580551 non-null int64LogInfo3 580551 non-null objectdtypes: int64(3), object(2)memory usage: 22.1+ MB 3.Userupdate_Info 借款人修改信息。 主要字段 描述 ListingInfo1 借款成交时间 UserupdateInfo1 修改内容 UserupdateInfo2 修改时间 idx 每一笔贷款的unique key 1234567891011train_userinfo.info()》&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 372463 entries, 0 to 372462Data columns (total 4 columns):Idx 372463 non-null int64ListingInfo1 372463 non-null objectUserupdateInfo1 372463 non-null objectUserupdateInfo2 372463 non-null objectdtypes: int64(1), object(3)memory usage: 11.4+ MB 比赛规则 参赛团队需要基于训练集数据构建预测模型，使用模型计算测试集的评分（评分数值越高，表示越有可能出现贷款违约）。 模型评价标准： 定义：本次比赛采用AUC来评判模型的效果。AUC即以False Positive Rate为横轴，True Positive Rate为纵轴的ROC （Receiver Operating Characteristic）curve下方的面积的大小。 $$AUC=\frac{\sum_{i} S_{i}}{M \times N}$$ 其中，M为正样本个数，N为负样本个数，M*N为正负样本对的个数。$S_{i}$为第i个正负样本对的得分，定义如下： $$S_{i}=\begin{cases} 1 &amp; score_{i-p} &gt; score_{i-n} \\ 0.5 &amp; score_{i-p} = score_{i-n} \\ 0 &amp; score_{i-p} &lt; score_{i-n} \end{cases}$$ 其中， $score_{i-p}$为正负样本对中，模型给正样本的评分， $score_{i-n}$为正负样本对中，模型给负样本的评分， AUC值在[0,1]区间，越高越好。 数据清洗 缺失值处理 删除缺失数据 Master表信息缺失情况相对比较为严重。 统计各字段的缺失情况： 123456null_sum = train_master.isnull().sum()null_sum = null_sum[null_sum!=0]null_sum_df = pd.DataFrame(null_sum, columns=['num'])null_sum_df['ratio'] = null_sum_df['num'] / 30000.0null_sum_df.sort_values(by='ratio', ascending=False, inplace=True)#对每个字段信息进行缺失率排序null_sum_df[:10] num ratio WeblogInfo_3 29030 0.967667 WeblogInfo_1 29030 0.967667 UserInfo_11 18909 0.630300 UserInfo_13 18909 0.630300 UserInfo_12 18909 0.630300 WeblogInfo_20 8050 0.268333 WeblogInfo_21 3074 0.102467 WeblogInfo_19 2963 0.098767 WeblogInfo_2 1658 0.055267 WeblogInfo_4 1651 0.055033 删除缺失严重的行: 1train_master.drop(['WeblogInfo_3', 'WeblogInfo_1', 'UserInfo_11', 'UserInfo_13', 'UserInfo_12', 'WeblogInfo_20'],axis=1,inplace=True) 删除缺失严重的行： 12345record_nan=train_master.isnull().sum(axis=1).sort_values(ascending=False)drop_record_index=[i for i in record_nan.loc[(record_nan&gt;=5)].index]print("Before train_master shape &#123;&#125;".format(train_master.shape))train_master.drop(drop_record_index, inplace=True)print("After train_master shape &#123;&#125;".format(train_master.shape)) 12Before train_master shape (30000, 222)After train_master shape (29189, 222) 缺失填充 填充缺失值要根据字段的属性进行合理填充，在没有字段信息的情况下，根据数据分布情况填充。 在缺失数据较少的情况下，用频率最高的值填充；在缺失数据较多的情况下，用均值填充。 12345678910111213141516171819202122232425print('Before all nan num&#123;&#125;'.format(train_master.isnull().sum().sum()))train_master.loc[train_master['UserInfo_2'].isnull(), 'UserInfo_2'] = '位置地点'train_master.loc[train_master['UserInfo_4'].isnull(), 'UserInfo_4'] = '位置地点'def fill_nan(f, method): if method == "most": common_value=pd.value_counts(train_master[f], ascending=False).index[0] else: common_value = train_master[f].mean() train_master.loc[train_master[f].isnull(), f]=common_value# 通过pd.value_counts(train_master[f])的观察得到经验fill_nan('UserInfo_1', 'most')fill_nan('UserInfo_3', 'most')fill_nan('WeblogInfo_2', 'most')fill_nan('WeblogInfo_4', 'mean')fill_nan('WeblogInfo_5', 'mean')fill_nan('WeblogInfo_6', 'mean')fill_nan('WeblogInfo_19', 'most')fill_nan('WeblogInfo_21', 'most')print('After all nan num: &#123;&#125;'.format(train_master.isnull().sum() .sum())) 12Before all nan num: 9808After all nan num: 0 异常值处理 本文在处理离群点时，先通过特征分类将数值型特征单独列出来。 通过画图的方式，发现离群点。 12345678910import seaborn as sbnsbn.set(style='whitegrid')import matplotlib.pyplot as plt%matplotlib inlinemelt = pd.melt(train_master, id_vars=['target'], value_vars=[f for f in numerical_features])#FacetGrid绘制各变量之间的关系图g = sbn.FacetGrid(data=melt, col="variable", col_wrap=4, sharex=False, sharey=False)g.map(sbn.stripplot, 'target', 'value', jitter=True, palette='muted') 针对每一个数值型特征值作图分析判断，并删除特征离群的对象。 1229189 lines before drop28074 lines after drop 删除数值型离群点，剩下28074个样本。 特征分类 特征可以分为二值特征、连续特征、枚举特征。 二值特征 主要是0/1特征，即特征只取两种值：0或者1 连续值特征 取值为有理数的特征，特征取值个数不定，例如距离特征，特征取值为是0~正无穷。 枚举值特征 主要是特征有固定个数个可能值，例如今天周几，只有7个可能值：周1，周2，…，周日。 这块内容主要是特征转换,本文涉及以下几种转换情况： 非二值特征转换为二值特征 检查所有特征值的分布，若出现频率最高的值占该特征所有取值情况高达50%，那么就可以将该特征转换为二值特征。 连续值特征转换为枚举特征 当连续值特征的取值范围过小时，可将每个值单独作为一类，即将连续值特征转换为枚举特征。 123456789101112131415161718192021222324252627282930"""-----------------------筛选二值特征--开始------------------"""ratio_threshold = 0.5binarized_features = []binarized_features_most_freq_value = []for f in train_master.columns: if f in ['target']: continue not_null_sum = (train_master[f].notnull()).sum() most_count = pd.value_counts(train_master[f], ascending=False).iloc[0] most_value = pd.value_counts(train_master[f], ascending=False).index[0] ratio = most_count / not_null_sum if ratio &gt; ratio_threshold: binarized_features.append(f) binarized_features_most_freq_value.append(most_value) """-----------------------筛选二值特征--结束------------------""""""-------------------筛选连续值特征--开始------------------"""numerical_features = [f for f in train_master.select_dtypes(exclude=['object']).columns if f not in (['Idx', 'target']) and f not in binarized_features] """-------------------筛选连续值特征--结束------------------""""""-------------------筛选枚举特征--开始------------------"""categorical_features = [f for f in train_master.select_dtypes (include=['object']).columns if f not in (['Idx', 'target']) and f not in binarized_features]"""------------------筛选枚举特征--结束------------------""" 数据转换 特征转换 123456789101112131415161718192021#将挑选出的特征转换为二值特征for i in range(len(binarized_features)): f = binarized_features[i] most_value = binarized_features_most_freq_value[i] train_master['b_'+f] = 1 train_master.loc[train_master[f] == most_value, 'b_'+f] = 0 train_master.drop([f], axis=1, inplace=True) #连续值特征转换为枚举特征import numpy as npfeature_unique_count = []for f in numerical_features: feature_unique_count.append((np.count_nonzero(train_master[f] .unique()), f))for c, f in feature_unique_count: if c &lt;= 10: print('&#123;&#125; moved from numerical to categorical'.format(f)) numerical_features.remove(f) categorical_features.append(f) 对数转换 为提高模型拟合程度，通常要求样本分布呈现(近似)正态分布。 去除离群点后的数值特征分布情况： 明显数据分布偏左，可以采取对数转换的方法将数据分布变得更均匀： 解析时间 解析时间，并将解析后的时间字段加入主表master信息中： 123456789101112131415161718import arrowdef parse_date(date_str, str_format='YYYY/MM/DD'): d = arrow.get(date_str, str_format) #月初、月中、月末 month_stage = int((d.day-1) / 10) + 1 return (d.timestamp, d.year, d.month, d.day, d.week, d.isoweekday(), month_stage)def parse_ListingInfo(date): d = parse_date(date, 'YYYY/M/D') return pd.Series(d, index=['ListingInfo_timestamp', 'ListingInfo_year', 'ListingInfo_month','ListingInfo_day', 'ListingInfo_week', 'ListingInfo_isoweekday', 'ListingInfo_month_stage'], dtype=np.int32)ListingInfo_parsed =train_master_['ListingInfo'].apply(parse_ListingInfo)print('before train_master_ shape&#123;&#125;'.format(train_master_.shape))train_master_ = train_master_.merge(ListingInfo_parsed, how='left', left_index=True, right_index=True)print('after train_master_ shape &#123;&#125;'.format(train_master_.shape)) 解析时间数据后，整个master表多了7列： 12before train_master_ shape (28074, 223)after train_master_ shape (28074, 230) 转换登陆信息 登陆信息： Idx Listinginfo1 LogInfo1 LogInfo2 LogInfo3 0 10001 2014-03-05 107 6 2014-02-20 1 10001 2014-03-05 107 6 2014-02-23 2 10001 2014-03-05 107 6 2014-02-24 3 10001 2014-03-05 107 6 2014-02-25 4 10001 2014-03-05 107 6 2014-02-27 统计每笔交易登陆次数、交易成功时间戳(包含借款、还款等)、登陆次数、登陆间隔时长、最新登陆时间戳、相同操作次数，得到转换后的登陆信息： loginfo_num loginfo_LogInfo1_unique_num XXX loginfo_LogInfo12_unique_num 0 3 26 XXX 8 1 5 11 XXX 4 2 8 125 XXX 13 3 12 199 XXX 11 4 16 15 XXX 7 转换修改信息 修改信息： Idx ListingInfo1 UserupdateInfo1 UserupdateInfo2 0 10001 2014/03/05 _EducationId 2014/02/20 1 10001 2014/03/05 _HasBuyCar 2014/02/20 2 10001 2014/03/05 _LastUpdateDate 2014/02/20 3 10001 2014/03/05 _MarriageStatusId 2014/02/20 4 10001 2014/03/05 _MobilePhone 2014/02/20 统计每笔交易修改信息次数、修改字段数、修改时间次数、间隔天数、最新修改时间戳、按照UserupdateInfo1信息分别统计，得到转换后的修改信息： Idx userinfo_num XXX userinfo_Position_num 0 3 13 XXX 0 1 5 13 XXX 0 2 8 14 XXX 0 3 12 14 XXX 0 4 16 13 XXX 0 枚举特征编码 pandas的get_dummies()函数： 参数： columns：要编码的DataFrame中的列名称。如果列为None，则将转换具有对象或类别dtype的所有列。 12345678910111213141516drop_columns= ['Idx', 'ListingInfo', 'UserInfo_20', 'UserInfo_19','UserInfo_8','UserInfo_7','UserInfo_4','UserInfo_2','ListingInfo_timestamp', 'loginfo_last_day_timestamp', 'userinfo_last_day_timestamp']train_master_ = train_master_.drop(drop_columns, axis=1)dummy_columns = categorical_features.copy()dummy_columns.extend(['ListingInfo_year', 'ListingInfo_month', 'ListingInfo_day', 'ListingInfo_week', 'ListingInfo_isoweekday', 'ListingInfo_month_stage'])finally_dummy_columns = []for c in dummy_columns: if c not in drop_columns: finally_dummy_columns.append(c)print('before get_dummies train_master_ shape &#123;&#125;'.format(train_master_.shape))train_master_ = pd.get_dummies(train_master_, columns=finally_dummy_columns)print('after get_dummies train_master_ shape &#123;&#125;'.format(train_master_.shape)) 建模工作前的数据处理工作基本就是以上内容，还包括合并三个表、删除不相关字段、添加at_home字段，增加家乡字段信息，可能与是否违约相关。 算法建模 标准化 创建训练集及验证集，并对训练集做标准化处理： 123456from sklearn.preprocessing import StandardScalerX_train = train_master_.drop(['target'], axis=1)X_train = StandardScaler().fit_transform(X_train)y_train = train_master_['target']print(X_train.shape, y_train.shape) 得到训练集及验证集的大小： 1(28074, 443) (28074,) 交叉验证 选择StratifiedKFoldS折交叉验证法，确保训练集中每一类的比例是相同的。 123from sklearn.model_selection import StratifiedKFoldcv = StratifiedKFold(n_splits=3, shuffle=True)#分层采样 分类算法评估 AUC、精确度、召回率： 12345678from sklearn.model_selection import cross_val_scoredef estimate(estimator, name='estimator'): auc = cross_val_score(estimator, X_train, y_train, scoring='roc_auc', cv=cv).mean() accuracy = cross_val_score(estimator, X_train, y_train, scoring='accuracy', cv=cv).mean() recall = cross_val_score(estimator, X_train, y_train, scoring='recall', cv=cv).mean() print("&#123;&#125;: auc:&#123;:f&#125;, recall:&#123;:f&#125;, accuracy:&#123;:f&#125;".format(name, auc, recall, accuracy)) 算法 评估以下分类算法： 线性分类器 LogisticRegression 广义线性分类器 RidgeClassifier 集成方法分类器 RandomForestClassifier、AdaBoostClassifier、XGBClassifier 支持向量机分类器 SVC、LinearSVC 通过评估方法选择分类效果较好的LogisticRegression、XGBClassifier、AdaBoostClassifier三种模型，最终通过投票的方式聚合三种模型。 123456789from sklearn.ensemble import VotingClassifierestimators = []estimators.append(('LogisticRegression', LogisticRegression()))estimators.append(('XGBClassifier', XGBClassifier(learning_rate=0.1, n_estimators=20, objective='binary:logistic')))estimators.append(('AdaBoostClassifier', AdaBoostClassifier()))voting = VotingClassifier(estimators = estimators, voting='soft')estimate(voting, 'voting') 最终达到的效果： 1voting: auc:0.800663, recall:0.001285, accuracy:0.944433]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据分析]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[背景 在数据分析的路上跌跌撞撞，从一开始实习做Splunk大数据分析，到小论文做时间序列聚类，再到现在为找工作丰富简历做金融项目“魔镜杯”风控算法大赛。这一路上，迂回转折，慢慢明白了，数据分析，是一个大的概念，包含的很多，但是都有一个共同的目标：帮助决策者做出明智的决策。 Splunk: Splunk软件主要用作大数据搜索引擎。 实习做项目的主要流程： 安装软件及配置，包括Enterprise(索引)、转发器以及部署配置，确保分散的数据能集中统一管理。 搜索数据，提取字段。 做一些基础的见解分析，再与客户沟通需求，定制化开发。 完成可视化报表以及告警等其他配套应用。 时间序列聚类： 这是自己小论文做的电力负荷聚类算法研究，通过阅读文献，大致知晓了数据清洗、特征选择、算法研究这些步骤，算法是写论文发表期刊的主要任务。但是，由于学术研究通常面对的数据集都是标准数据集，不存在数据缺失以及噪声的情况，而现实世界里的数据繁杂、缺失、异常随处可见，对数据的预处理以及特征工程反而是比研究算法更为重要的事情。 项目经验 感觉自己整个研究生生涯顺序都是反的，先去了实习，而延误了写论文。先看文献，而后做试验。如果有一次重来的机会，我会先做学术工作，把论文搞定后再去做相关算法的工作，而不是虎头虎脑地去实习。先动手做实验，而不是看文献，还是中文的论文，简直就是垃圾，误导祖国的花朵。目前想找金融方面的数据分析工作，所以在网上找到了2016年拍拍贷“魔镜杯”风控算法大赛的项目。通过学习各位获奖大神的研究现实问题的思路，这才茅塞顿开，豁然开朗，感觉找到了做数据分析的正轨。 数据分析 参考：https://www.cnblogs.com/charlotte77/p/5606926.html 数据分析的常见流程如下： 关键步骤： 数据清洗 特征工程 算法模型 本文主要以2016年拍拍贷主办的“魔镜杯”风控算法大赛项目介绍以上三个步骤，熟悉数据分析的流程。 数据清洗 数据清洗主要包含： 缺失值处理 异常值处理 去重处理 噪音数据处理 为何要数据清洗？ 平常在论文里用到的公开数据集譬如UCI标准数据集都是没有缺失值、异常值、噪音的优质数据。 然而在现实世界里，我们要探索的数据往往都是包含了大量缺失值、噪音的数据，例如，app里一些个人信息，分为标*必填字段，以及非必填字段，那么这样就会存在信息缺失了，同时，部分信息可能因为手工误填而导致出现异常值。而这些低质量的数据会对后序的算法建模产生巨大的影响，直接导致模型的偏差。 所以我们必须要进行数据清洗工作。 缺失值处理 没有高质量的数据，就没有高质量的数据挖掘结果，数据值缺失是数据分析中经常遇到的问题之一。 处理不完整数据集的方法主要有三大类：删除元组、数据补齐、不处理。 删除元组 将存在遗漏信息属性值的对象（元组，记录）删除，从而得到一个完备的信息表。这种方法简单易行，在对象有多个属性缺失值、被删除的含缺失值的对象与初始数据集的数据量相比非常小的情况下非常有效。 缺点：对于小样本数据集，即使删除少量数据也可能严重影响结果的正确性。 数据补齐 用一定的值去填充缺失值，从而使信息表完备化。通常基于统计学原理，根据初始数据集中其余对象取值的分布情况来对一个缺失值进行填充。数据挖掘中常用的有以下几种补齐方法： 人工填写(不可行) 当数据规模很大、空值很多时 特殊值填充(不可行)，该方法耗时耗力，不可行。 将空值作为一种特殊的属性值来处理，它不同于其他的任何属性值。如所有的空值都用“unknown”填充。这样将形成另一个有趣的概念，可能导致严重的数据偏离，一般不推荐使用。 平均值填充 将初始数据集中的属性分为数值属性和非数值属性来分别进行处理。 如果空值是数值型的，就根据该属性在其他所有对象的取值的平均值来填充该缺失的属性值； 如果空值是非数值型的，就根据统计学中的众数原理，用该属性在其他所有对象的取值次数最多的值(即出现频率最高的值)来补齐该缺失的属性值。与其相似的另一种方法叫条件平均值填充法（Conditional Mean Completer）。在该方法中，用于求平均的值并不是从数据集的所有对象中取，而是从与该对象具有相同决策属性值的对象中取得。这两种数据的补齐方法，其基本的出发点都是一样的，以最大概率可能的取值来补充缺失的属性值，只是在具体方法上有一点不同。与其他方法相比，它是用现存数据的多数信息来推测缺失值。 下面举例说明： 1. “年收入”：商品推荐场景下填充平均值，借贷额度场景下填充最小值； “行为时间点”：填充众数； “价格”：商品推荐场景下填充最小值，商品匹配场景下填充平均值； “人体寿命”：保险费用估计场景下填充最大值，人口估计场景下填充平均值； “驾龄”：没有填写这一项的用户可能是没有车，为它填充为0较为合理； ”本科毕业时间”：没有填写这一项的用户可能是没有上大学，为它填充正无穷比较合理； “婚姻状态”：没有填写这一项的用户可能对自己的隐私比较敏感，应单独设为一个分类，如已婚1、未婚0、未填-1。 插补法 随机插补法 从总体中随机抽取某个样本代替缺失样本 多重插补法 通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理 热卡插补 指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。 优点：简单易行，准去率较高 - 缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补 拉格朗日差值法和牛顿插值法 建模法 回归 基于完整的数据集，建立回归方程。对于包含空值的对象，将已知属性值代入方程来估计未知属性值，以此估计值来进行填充。 缺点：当变量不是线性相关时会导致有偏差的估计。 - K最近距离邻法（K-means clustering） 先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。 期望值最大化方法 EM算法是一种在不完全数据情况下计算极大似然估计或者后验分布的迭代算法。在每一迭代循环过程中交替执行两个步骤：E步（Excepctaion step,期望步），在给定完全数据和前一次迭代所得到的参数估计的情况下计算完全数据对应的对数似然函数的条件期望；M步（Maximzation step，极大化步），用极大化对数似然函数以确定参数的值，并用于下步的迭代。算法在E步和M步之间不断迭代直至收敛，即两次迭代之间的参数变化小于一个预先给定的阈值时结束。该方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。 归纳法 - C4.5方法 通过寻找属性间的关系来对遗失值填充。它寻找之间具有最大相关性的两个属性，其中没有遗失值的一个称为代理属性，另一个称为原始属性，用代理属性决定原始属性中的遗失值。这种基于规则归纳的方法只能处理基数较小的名词型属性。 就几种基于统计的方法而言，删除元组法和平均值法差于热卡填充法、期望值最大化方法和多重填充法；回归是比较好的一种方法，但仍比不上hot deck和EM；EM缺少MI包含的不确定成分。值得注意的是，这些方法直接处理的是模型参数的估计而不是空缺值预测本身。它们合适于处理无监督学习的问题，而对有监督学习来说，情况就不尽相同了。譬如，你可以删除包含空值的对象用完整的数据集来进行训练，但预测时你却不能忽略包含空值的对象。另外，C4.5和使用所有可能的值填充方法也有较好的补齐效果，人工填写和特殊值填充则是一般不推荐使用的。 不处理 补齐处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实，在对不完备信息进行补齐处理的同时，我们或多或少地改变了原始的信息系统。而且，对空值不正确的填充往往将新的噪声引入数据中，使挖掘任务产生错误的结果。因此，在许多情况下，我们还是希望在保持原始信息不发生变化的前提下对信息系统进行处理。 不处理缺失值，直接在包含空值的数据上进行数据挖掘的方法包括贝叶斯网络和人工神经网络等。 贝叶斯网络提供了一种自然的表示变量间因果信息的方法，用来发现数据间的潜在关系。在这个网络中，用节点表示变量，有向边表示变量间的依赖关系。贝叶斯网络仅适合于对领域知识具有一定了解的情况，至少对变量间的依赖关系较清楚的情况。否则直接从数据中学习贝叶斯网的结构不但复杂性较高（随着变量的增加，指数级增加），网络维护代价昂贵，而且它的估计参数较多，为系统带来了高方差，影响了它的预测精度。只有当数据集较小或满足某些条件（如多元正态分布）时完全贝叶斯分析才是可行的。 人工神经网络可以有效的对付缺失值，但人工神经网络在这方面的研究还有待进一步深入展开。 映射到高维空间 比如性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。 而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差 异常值处理 异常值通常也称为“离群点”。 发现异常值 画图 简单的统计分析 1data.describe()#pandas的describe()函数 3∂原则 箱型图分析 基于模型检测 首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象 优缺点：1.有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；2.对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。 基于距离 通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象 优缺点：1.简单；2.缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；3.该方法对参数的选择也是敏感的；4.不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。 基于密度 当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。 优缺点：1.给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；2.与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；3.参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。 基于聚类 基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。 优缺点：1.基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；2.簇的定义通常是离群点的补，因此可能同时发现簇和离群点；3.产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；4.聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。 异常值处理 删除异常值 明显看出是异常且数量较少可以直接删除 不处理 如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。 平均值替代 损失信息小，简单高效。 视为缺失值 可以按照处理缺失值的方法来处理]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SQL]]></title>
    <url>%2F2018%2F08%2F07%2FMySQL%2F</url>
    <content type="text"><![CDATA[参考：http://www.runoob.com/sql/sql-syntax.html 数据仓库 四个特点 面向主题 操作型数据库的数据组织面向事务处理任务，各个业务系统之间各自分离，而数据仓库中的数据是按照一定的主题域进行组织。主题是一个抽象的概念，是指用户使用数据仓库进行决策时所关心的重点方面，一个主题通常与多个操作型信息系统相关。 集成的 面向事务处理的操作型数据库通常与某些特定的应用相关，数据库之间相互独立，并且往往是异构的。而数据仓库中的数据是在对原有分散的数据库数据抽取、清理的基础上经过系统加工、汇总和整理得到的，必须消除源数据中的不一致性，以保证数据仓库内的信息是关于整个企业的一致的全局信息。 相对稳定的 操作型数据库中的数据通常实时更新，数据根据需要及时发生变化。数据仓库的数据主要供企业决策分析之用，所涉及的数据操作主要是数据查询，一旦某个数据进入数据仓库以后，一般情况下将被长期保留，也就是数据仓库中一般有大量的查询操作，但修改和删除操作很少，通常只需要定期的加载、刷新。 反映历史变化 操作型数据库主要关心当前某一个时间段内的数据，而数据仓库中的数据通常包含历史信息，系统记录了企业从过去某一时点(如开始应用数据仓库的时点)到目前的各个阶段的信息，通过这些信息，可以对企业的发展历程和未来趋势做出定量分析和预测。 常用模型 维度模型 星型模型 雪花模型 星座模型 关系模型 范式建模 数据库基础 数据库操作语言(Data Manipulation Language, DML) 它们是SELECT、UPDATE、INSERT、DELETE，就象它的名字一样，这4条命令是用来对数据库里的数据进行操作的语言。 数据库定义语言(Data Definition Language, DDL) DDL比DML要多，主要的命令有CREATE、ALTER、DROP、TRUNCATE等，DDL主要是用在定义或改变表（TABLE）的结构，数据类型，表之间的链接和约束等初始化工作上，他们大多在建立表时使用。 数据库控制语言(Data Control Language, DCL) 用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke等）语句。在默认状态下，只有sysadmin,dbcreator,db_owner或db_securityadmin等人员才有权力执行DCL语句。 查询筛选 表World: name continent area population gdp Afghanistan Asia 652230 25500100 20343000 Albania Europe 28748 2831741 12960000 Algeria Africa 2381741 37100000 188681000 Andorra Europe 468 78115 3712000 Angola Africa 1246700 20609294 100990000 通过SQL查询语句 123SELECT name, population, areaFROM WorldWHERE population &gt; 25000000 得到查询结果： name population area Afghanistan 25500100 652230 Algeria 37100000 2381741 处理重复数据 表Person: Id Email 1 a@b.com 2 c@d.com 3 a@b.com 选出重复邮件： 1234select Emailfrom Persongroup by Emailhaving count(*) &gt; 1 得到： Email a@b.com 排序 表 cinema: id movie description rating 1 War great 3D 8.9 2 Science fiction 8.5 3 irish boring 6.2 4 Ice song Fantacy 8.6 5 House card Interesting 9.1 筛选出id为奇数且描述为不无聊的，按照评分由高到低： 1234SELECT *FROM cinemaWHERE (id % 2 = 1) AND (description &lt;&gt; &apos;boring&apos;)#&lt;&gt;不等于，筛选条件有歧义一定要加括号ORDER BY rating DESC 结果： id movie description rating 5 House card Interesting 9.1 1 War great 3D 8.9 更新表 表salary: id name sex salary 1 A m 2500 2 B f 1500 3 C m 5500 4 D f 500 将sex互换： 12UPDATE salary SET sex= CHAR(ASCII(&apos;f&apos;)+ASCII(&apos;m&apos;)-ASCII(sex)); id name sex salary 1 A f 2500 2 B m 1500 3 C f 5500 4 D m 500 删除 DELETE、DROP、TRUNCATE都可以删除表内的数据。 不同的是： TRUNCATE和DELETE只删除数据不删除表的结构 DROP语句将删除表的结构，被依赖的约束(constrain)、触发器(trigger)、索引(index)；依赖于该表的存储过程/函数将保留,但是变为 invalid 状态。 DELETE是DML语句，DROP和TRUNCATE是DDL语句 DELETE语句是数据库操作语言(DML),这个操作会放到 rollback segement 中，事务提交之后才生效；如果有相应的 trigger，执行的时候将被触发。 DROP和TRUNCATE是数据库定义语言(DDL),操作立即生效，原数据不放到 rollback segment 中，不能回滚，操作不触发 trigger。 DELETE语句不会自动提交，DROP和TRUNCATE执行后会自动提交，所以要小心使用DROP和TRUNCATE语句。 速度：DROP&gt; TRUNCATE &gt; DELETE DELETE DELETE语句是DML语句，不会自动提交，万一操作失误，还可以通过回滚恢复数据。 不带WHERE子句的DELETE语句与TRUNCATE的功能相同，两者都是删除表中的全部行，但是TRUNCATE TABLE比DELETE速度快，且使用的系统和事务日志资源少。DELETE语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 DROP drop 语句将表所占用的空间全部释放。 drop 语句将删除表的结构被依赖的约束(constrain)、触发器(trigger)、索引(index)；依赖于该表的存储过程/函数将保留,但是变为 invalid 状态。 TRUNCATE TRUNCATE TABLE删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用DELETE。如果要删除表定义及其数据，请使用DROP TABLE 语句。 TRUNCATE TABLE删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用DELETE。如果要删除表定义及其数据，请使用DROP TABLE 语句。 连接 等值连接 在连接条件中使用等于号(=)运算符比较被连接列的列值，其查询结果中列出被连接表中的所有列，包括其中的重复列。 不等值连接 在连接条件使用除等于运算符以外的其它比较运算符比较被连接的列的列值。这些运算符包括&gt;、&gt;=、&lt;=、&lt;、!&gt;、!&lt;和&lt;&gt;。 自然连接 在连接条件中使用等于(=)运算符比较被连接列的列值，但它使用选择列表指出查询结果集合中所包括的列，并删除连接表中的重复列。 示例： book表： id bookname sutid 1 python编程 1 2 sql编程 2 3 winform 3 4 web教程 4 6 前端教程 10 stu表： sutid stuname 1 张三 2 李四 3 王五 4 任六 5 赵七 内连接 内连接查询操作列出与连接条件匹配的数据行，它使用比较运算符比较被连接列的列值。 123select * from book as a,stu as b where a.sutid = b.stuidselect * from book as a inner join stu as b on a.sutid = b.stuid id bookname sutid sutid stuname 1 python编程 1 1 张三 2 sql编程 2 2 李四 3 winform 3 3 王五 4 web教程 4 4 任六 外连接 左连接 以左表为基准，将a.stuid = b.stuid的数据进行连接，然后将左表没有的对应项显示，右表的列为NULL 1select * from book as a left join stu as b on a.sutid = b.stuid id bookname sutid sutid stuname 1 python编程 1 1 张三 2 sql编程 2 2 李四 3 winform 3 3 王五 4 web教程 4 4 任六 6 前端教程 10 NULL NULL 右连接 以右表为基准，将a.stuid = b.stuid的数据进行连接，然以将右表没有的对应项显示，左表的列为NULL 1select * from book as a right join stu as b on a.sutid = b.stuid id bookname sutid sutid stuname 1 python编程 1 1 张三 2 sql编程 2 2 李四 3 winform 3 3 王五 4 web教程 4 4 任六 NULL NULL NULL 5 赵七 全连接 完整外部联接返回左表和右表中的所有行。当某行在另一个表中没有匹配行时，则另一个表的选择列表列包含空值。如果表之间有匹配行，则整个结果集行包含基表的数据值。 1select * from book as a full outer join stu as b on a.sutid = b.stuid id bookname sutid sutid stuname 1 python编程 1 1 张三 2 sql编程 2 2 李四 3 winform 3 3 王五 4 web教程 4 4 任六 6 前端教程 10 NULL NULL NULL NULL NULL 5 赵七]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python]]></title>
    <url>%2F2018%2F08%2F06%2FPython%2F</url>
    <content type="text"><![CDATA[本文主要用于记录LeetCode中遇到的比较好的一些python写法以及招聘笔试面试中遇到的问题。 self参数 在python的类的方法定义中，请描述’self’参数的作用？ self参数在类定义中，指类本身，可以用来传递外部参数。 map函数 描述： map()会根据提供的函数对指定序列做映射。 语法： 1map(function, iterable, ...) 参数: function – 函数，有两个参数 iterable – 一个或多个序列 返回值： Python 3.x 返回迭代器。 实例： 123456789101112def square(x) : # 定义计算平方数的函数 return x ** 2 map(square, [1,2,3,4,5]) # 计算列表各个元素的平方》[1, 4, 9, 16, 25]map(lambda x: x ** 2, [1, 2, 3, 4, 5]) # 使用 lambda 匿名函数》[1, 4, 9, 16, 25]# 提供了两个列表，对相同位置的列表数据进行相加map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])[3, 7, 11, 15, 19] 总结： map(function, iterable, …) 相当于[f(x) for x in list] sum函数 描述： sum() 方法对系列进行求和计算。 语法: sum(iterable[, start]) 参数: iterable – 可迭代对象，如：列表、元组、集合。 start – 指定相加的参数，如果没有设置这个值，默认为0。 返回值: 返回计算结果。 实例: 12345678sum([0,1,2])3sum((2, 3, 4), 1)10sum([0,1,2,3,4], 2) 12 常见用法： 将程序变得更pythic，以171.Excel表列序号为例： 常规 1234res = 0 for index in range(len(s)): res += (26 ** (len(s) - index - 1)) * (ord(s[index]) - ord('A') + 1) return res sum() 1return sum((26 ** (len(s) - index - 1)) * (ord(s[index]) - ord('A') + 1) for index in range(len(s))) 把每次迭代的结果以sum的方式一次性取和。 ord函数 描述： 它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值，如果所给的 Unicode 字符超出了你的 Python 定义范围，则会引发一个 TypeError 的异常。 语法： 1ord(c) 参数： c – 字符 返回值： 返回值是对应的十进制整数。 实例： 123456ord('a')》97ord('b')》98ord('c')》99 排序 数组倒序 123a = [1, 2, 3, 4, 5]a[::-1]》[5, 4, 3, 2, 1] sort函数 sort()是Python列表(仅适用于列表)的一个内置的排序方法，list.sort()方法排序时直接修改原列表，返回None； 语法： 1list.sort(*, key=None, reverse=None) 参数： key 带一个参数的函数，返回一个值用来排序，默认为 None reverse 表示排序结果是否反转，默认是升序 实例： 123456a=['1',1,'a',3,7,'n']print a.sort()》Nonea》[1, 3, 7, '1', 'a', 'n'] sorted函数 sorted()是迭代器(不仅针对列表)的排序方法，可以保留原列表，返回新列表。 语法： 1sorted(iterable[, key][, reverse]) 参数： key 带一个参数的函数，返回一个值用来排序，默认为 None reverse 表示排序结果是否反转 实例： 123a = (1,2,4,2,3)print(sorted(a))》[1, 2, 2, 3, 4] set 数据类型set是一个无序不重复元素集。 基本功能包括关系测试和消除重复元素。 集合对象支持联合、交、差和对称差集等数学运算。 支持： x in set len(set) for x in set 不支持： indexing slicing 其它类序列（sequence-like）的操作 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960x = set('spam')y = set(['h','a','m'])x, y》(set(['a', 'p', 's', 'm']), set(['a', 'h', 'm']))x &amp; y # 交集》set(['a', 'm'])x | y # 并集x.union(y)》set(['a', 'p', 's', 'h', 'm'])x - y # 差集 在x中但不在y中》set(['p', 's'])x ^ y #对称差集 在x或y中，但不会同时出现在二者中》set(['p', 's'])#去除重复元素：a = [11,22,33,44,11,22]set(a)》set([33, 11, 44, 22])# 添加一项x.add('x')# 添加多项x.update([10,37,42]) # 删除一项,如果不存在就报错x.remove('H')# 删除一项，存在就删除x.discard('H')# 删除 set 中的所有元素x.clear() # 删除并且返回 set “s”中的一个不确定的元素, 如果为空则报错s.pop()# set 的长度len(x)# 测试 s 是否是 x 的成员s in x# 测试 s 是否不是 x 的成员s not in x# 测试是否 y 中的每一个元素都在 x 中s.issubset(t)s &lt;= t# 测试是否 y 中的每一个元素都在 x 中s.issuperset(t)s &gt;= t# 返回 set “s”的一个浅复制s.copy() zip函数 描述： 从参数中的多个迭代器取元素组合成一个新的迭代器。 语法： 1zip(iterable,[iterable...]) 注意： 当有多个参数时，zip(a,b)中a和b的维数相同，若不相同时，取最小的维数进行组合。 参数： 一个或多个迭代器，例如，元组、列表、字典等。 返回值： 返回一个zip对象，其内部元素为元组； 可以转化为元组、列表、字典 实例： 单个参数 123456list1 = [1, 2, 3, 4]tuple1 = zip(list1)print("zip()函数的返回类型：\n", type(tuple1))》&lt;class 'zip'&gt; print("zip对象转化为列表：\n", list(tuple1))》[(1,), (2,), (3,), (4,)] 多个参数 123456789m = [[1,2,3], [4,5,6], [7,8,9]] n = [[2,2,2], [3,3,3], [4,4,4]] p = [[2,2,2], [3,3,3,]list(zip(m, n))》[([1, 2, 3], [2, 2, 2]), ([4, 5, 6], [3, 3, 3]), ([7, 8, 9], [4, 4, 4])]list(zip(m, p))》[([1, 2, 3], [2, 2, 2]), ([4, 5, 6], [3, 3, 3])] *zip(*iterables) 描述： *zip()函数是zip()函数的逆过程，将zip对象变成原先组合前的数据。 实例： 1234567m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]n = [[2, 2, 2], [3, 3, 3], [4, 4, 4]]m2, n2 = zip(*zip(m, n))m2》[[1, 2, 3], [4, 5, 6], [7, 8, 9]]n2》[[2, 2, 2], [3, 3, 3], [4, 4, 4]] zip(*) 12p = [[1,2,3],[4,5,6]]zip(*p)# 等价于zip([1,2,3],[4,5,6])，相当于把p最外层的[]给去掉了 sys模块 sys.stdin 标准输入： stdin获得输入(用input()输入的通过它获得) readline() 每次读取一整行，包括 “\n” 字符。 readlines() 读取所有行并返回列表,包括 “\n” 字符。 sys.stdout stdout用于输出print()和expression语句以及input()的提示。 print()的本质就是sys.stdout.write(object + ‘\n’) strip函数 移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。 注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。 语法： 1str.strip([chars]) 参数: chars:移除字符串头尾指定的字符序列。 返回值: 返回移除字符串头尾指定的字符生成的新字符串。 实例： 123456789# 去除首尾字符 0str = "00000003210Runoob01230000000";print(str.strip('0'))》3210Runoob0123# 去除首尾空格str2 = " Runoob "print(str2.strip())》Runoob]]></content>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow]]></title>
    <url>%2F2018%2F08%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[TensorFlow是一个机器学习框架，它提供了一门机器学习速成课程。 官方网址：https://developers.google.cn/machine-learning/crash-course/ 本文按照谷歌提供的机器学习速成课程记录学习笔记。 线性回归 基本概念 下图展示了蟋蟀每分钟的鸣叫声与温度（摄氏度）的关系。 为了清楚地显示鸣叫声与温度之间的关系，创建公式： $$y=mx+b$$ 其中： $y$:指的是温度（以摄氏度表示），即我们试图预测的值。 $m$:指的是直线的斜率。 $x$:指的是每分钟的鸣叫声次数，即输入特征的值。 $b$:指的是 y 轴截距。 将上面代数公式，转换为机器学习样式的方程： $$y^{’}=b+w_{1}x_{1}$$ 其中： $y_{’}$:指的是预测标签（理想输出值）。 $b$:指的是偏差（y 轴截距）。而在一些机器学习文档中，它称为 。 $w_{1}$:指的是特征 1 的权重。权重与上文中用 表示的“斜率”的概念相同。 $x_{1}$:指的是特征（已知输入项）。 延伸到多个特征的情况： $$y^{’}=b+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}$$ 损失 平方损失 平方损失又叫$L_{2}$损失。 红色箭头表示损失 蓝线表示预测 单个样本的平方损失：$${ \left( y-{ y }^{ ’ } \right) }^{ 2 }$$ 均方误差 均方误差 (MSE) 指的是每个样本的平均平方损失。 $$MSE=\frac{1}{N} \sum_{(x,y) \in D} {(y-prediction(x))^{2}}$$ 其中： $(x,y)$指的是样本，其中: $x$指的是模型进行预测时使用的特征集（例如，温度、年龄和交配成功率。 $y$指的是样本的标签(例如，每分钟的鸣叫次数)。 $prediction(x)$指的是权重和偏差与特征集$x$结合的函数。 $D$指的是包含多个有标签样本(即$(x,y)$)的数据集。 $N$指的是$D$中的样本数量。 虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。 降低损失 迭代方法 下图显示了机器学习算法用于训练模型的迭代试错过程： 我们应该为$b$和$w_{1}$设置哪些初始值？ $$y^{’}=b+w_{1}x_{1}$$ 通过迭代方式来降低损失，就是不停的试错，直到得到损失最小的模型。 梯度下降法 根据迭代算法的思想，计算$w_{1}$所有可能值的损失。 损失与$w_{1}$的图形始终是凸形的，随着$w_{1}$的增大，损失从大变小，再变大的一个过程。 凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。 通过计算整个数据集中 每个可能值的损失函数来找到收敛点这种方法效率太低。 我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为梯度下降法。 梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。 梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。 如果您选择的学习速率过小，就会花费太长的学习时间。 相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样。 随机梯度下降法 在梯度下降法中，批量指的是用于在单次迭代中计算梯度的样本总数。到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。 包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。 如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。 小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。 TF的基本介绍 TensorFlow工具包层次结构： 不同层的用途： 工具包 说明 Estimator (tf.estimator) 高级 OOP API tf.layers/tf.losses/tf.metrics 用于常见模型组件的库 TensorFlow 低级 API TensorFlow 由以下两个组件组成： 图协议缓冲区 执行（分布式）运行 这两个组件类似于 Java 编译器和 JVM。正如 JVM 会实施在多个硬件平台（CPU 和 GPU）上一样，TensorFlow 也是如此。 tf.estimator API tf.estimator API是TF的高级API，代码行数少，易于使用，但是设计不够灵活。 tf.estimator 与 scikit-learn API 兼容。 本文中主要使用tf.estimator API学习TensorFlow。]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark入门]]></title>
    <url>%2F2018%2F07%2F29%2FSpark%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[继Spark安装教程之后，这是一篇Spark入门教程，记录我学习Spark的思路。 背景： 使用Python、Pandas、Numpy学习机器学习、数据分析、数据挖掘 论文瓶颈之时，偶然决定学习Spark，算是真正入了大数据的门，毕竟符合了数据量“大”的要求 RDD、DataFrame、Datasets 安装配置完Spark相关软件后，便开始照着官网教程http://spark.apache.org/docs/2.3.0/开始依葫芦画瓢😑 最先接触RDD(resilient distributed dataset,弹性分布式数据集)，如其名一般，专为分布式计算而生的一种数据类型。但是它有一个比较明显的缺点，就是不知道RDD数据里到底是什么，也可能我刚入门的原因。 DataFrame确实可以提供详细的信息，使用Spark SQL可以知晓数据内容，与之前在Pandas里的DataFrame名字一样，是不是有某种联系😵 Datasets是支持scala或者java开发的数据API，看大佬博客里解析说Datasets其实也就是DataFrame的一个特例，那么好吧，先暂时放弃你😂毕竟学Scala或者Java都挺费劲的，我还是多研究支持多种语言接口的DataFrame吧 未完待续…]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark安装配置]]></title>
    <url>%2F2018%2F07%2F23%2FSpark%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[背景 这是论文的坑😢到现在小论文还没有着落，沦落到害怕能不能毕业了😣 积极的想法：趁这个机会对大数据工作有所了解，找工作的时候又可以多以一个吹的话题了。 安装 安装Spark前必须要安装3个软件：JDK、Scala、Hadoop，一定要注意各个软件的安装与配置，否则直接影响Spark能否成功安装。 这里提一下，建议所有的软件都安装在没有空格的路径里，比如D:\Programe files就很可能报错。 JDK Java环境，必须安装1.8版本以上。之前电脑上本来就有JDK环境，不过是1.7的，一开始没在意，导致后面Spark安装后出现乱码的情况。所以一定要更新到1.8以上。 安装步骤很简单直接下载对应系统版本的msi文件，一路next，自动帮你配置好环境。 Windows系统64位1.8版本JDK安装包jdk-8u181-windows-x64： 链接：https://pan.baidu.com/s/1NhA5L3py-GOT-ejNbLxF6Q 密码：06da 检验是否安装成功： Scala Scala的版本一定要跟Spark相匹配，Spark下载官网明确指出： 1Note: Starting version 2.0, Spark is built with Scala 2.11 by default. Scala 2.10 users should download the Spark source package and build with Scala 2.10 support. 2.0版本及以上的Spark都是默认以Scala2.11版本为基础的，所以下载Scala的时候也要选择对应的版本。 Windows系统64位scala-2.11.7msi安装包： 链接：https://pan.baidu.com/s/1PH56iYE6qxDo9VyIvbogCA 密码：nkzq 跟JDK一样，因为都是MSI文件安装，不需要管PATH路径的问题。 检验是否安装成功： Hadoop 下载Spark安装包spark-2.3.0-bin-hadoop2.7明确指出了适配hadoop2.7版本的Hadoop。 下载hadoop-2.7.6.tar安装包，解压到指定目录。 链接：https://pan.baidu.com/s/1oAc7LYNSi_F70AjK7NTfAQ 密码：q1nv 因为Hadoop的bin目录下没有winutils.exe文件，可能还会报一个错误，解决方案： 下载hadoop win工具 地址：https://github.com/srccodes/hadoop-common-2.2.0-bin 将winutils.exe复制到 D:\software\hadoop-2.7.6\bin目录下 设置环境变量。 Spark 这里可以先安装Spark,再安装Hadoop。 Spark都是压缩安装包，选择spark-2.3.0-bin-hadoop2.7： 链接：https://pan.baidu.com/s/1Kz1iPGhEqfWMeax0mz7E0A 密码：23ii 下载好后解压到SPARK_HOME目录(自定义，不建议有空格的路径)，添加环境变量： 报错 按照之前的步骤安装配置，现在来检验下Spark是否正确安装了。 检验是否安装成功： 报错了:sad:好吧···开始漫长的解决报错路程 无法加载本地hadoop库 1WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 这个问题暂时还没有解决，暂时也没有对我的学习造成困扰。 感觉要学习Spark，不能单独只学习Spark，还要结合Hadoop、Hive等一起学习，才能对大数据分布式平台应用有一个整体的概念。 配置 Jupyter notebook+Pyspark 数据分析工具利器——Jupyter notebook 它实际就是一个类似IDE的东西，特点： 方便快捷，浏览器随时打开使用，无需等待，尤其是跟PyCharm相比。 便于梳理思路，它独特的cell方式，可以让每一步的结果显示在页面上，并不会覆盖后序的步骤，有利于数据分析工作的呈现。 而我安装使用Spark的目的，也就是用它实现大数据的分析工作，原本单机环境，最多跑个几千条数据就OUTOFMEMORY了😴 前提： 必须已经安装了Jupyter notebook。 设置环境变量，名称和值对应如下： 配置完成后就可以在CMD中输入pyspark，自动弹出浏览器使用pyspark啦😂]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode作战计划]]></title>
    <url>%2F2018%2F06%2F19%2FLeetCode%E4%BD%9C%E6%88%98%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[总规划 论文初稿完成，目前处于投递期刊状态😐看看各大公司招聘预告😱7月中旬开始校招，想想真是没时间喘息啊🙅是时候好好规划下了 规划Version1 预期4周的时间完成LeetCode绝大部分题型，熟悉总结解题规律，到7月中旬就可以尝试去参加笔试面试了😝 123406/19--06/24: 数组(10)、动态规划(10)06/25--07/01: 字符串(10)、数学(10)07/02--07/08: 哈希表(10)、树(10)07/09--07/15: 深度优先搜索(10)、二分查找(5) 目前的计划表就是这样，后期看情况调整，每周日来此汇报进度，并针对每种题型单独写博客整理解题思路。 按照上面的计划执行LeetCode一周后，每周做两组题型对我来说，难度颇大，没办法按时完成任务😔参考网上意见后决定暂时放弃LeetCode路线，LeetCode适合原本就是编程大神的人或者工作过一段时间的，并不适合小白类型的选手😲 决定换牛客网了，补充下基础知识，再去做算法题。牛客网有数据结构、算法等基础知识相关的选择题，做一做选择题复习下基础知识，还有招聘真题，牛客网应该更适合我循序渐进得准备笔试。 规划Version2 2018/6/25 12306/25--07/01: [数据结构]树、栈；[算法]排序(5)07/02--07/08: [数据结构]图、链表；[算法]复杂度(5)07/09--07/15: [数据结构]哈希、队列；[算法]查找(5) 规划Version3 感觉在写这第三个版本计划时，就是分分钟打脸的写照😑 之前规划7月中旬出去找实习，却被突然的中文核心等待一个月退稿而落幕。整个7月都在修改翻译论文投会议，也不知道有没有机会中。不管结果怎么样，小论文在研究生期间就这样了，也算是尽力了吧，整整从18年1月到7月，除去寒假过年，半年时间都在做论文这件事情。 目前找工作的进度最快也要到月底了，2周的时间，集中火力做前200道LeetCode中easy题目，快点出去面试，积攒面经，希望努力能找个大公司工作吧 总结 数据结构 划分依据 数据类型 举例 集 合 无逻辑关系 逻辑 线性结构 一维数组、队列、栈 非线性结构 树、图、网 顺序存储结构 存储 链式存储结构 索引存储结构 散列存储结构 数组 日期：2018/06/22 16:15 一共13道题目，每道题目中往往结合固定几种方法解题，规律性较强，总结题目、标签对应如下： 题号 数组 哈希表 双指针 二分查找 分治算法 1 ★ ★ 4 ★ ★ ★ 11 ★ ★ 15 ★ ★ 16 ★ ★ 18 ★ ★ ★ 26 ★ ★ 283 ★ ★ 448 ★ 485 ★ 561 ★ 566 ★ 766 ★ 由此可以看出，数组类型的题目常与双指针、哈希表方法相结合。 统计完成情况： 13题中通过5题，正确率只有38%😭其中6道双指针题目，只会其中1题，双指针类型题目错误率占比38%😶也就是说数组题目中基本碰到双指针就基本都不会😶😶😶 题号 完成情况 1 ★ 4 ★ 11 题目旨意模糊，答案为返回最大容量 15 不会去重 16 无法将想法转换为编程语言 18 复杂 26 题目旨意模糊，不需要删除重复项，只要没有引用到即可 283 ★ 448 ★ 485 思路 561 ★ 566 特别函数 766 思路 特殊函数 第566题用到的两个函数，平常用的比较少： 1.降低数组维度 1sum([[1,2], [3,4]], []) 输出： 1[1,2,3,4] 2.压缩函数zip() 常规考虑 1.对部分数组题目进行排序操作，可以减少程序运行时间，降低复杂度。 1nums.sort() 树 日期：2018/06/27 20:57 转战牛客网后的第一次总结，牛客网上可以更好地巩固复习基础知识，选择题的正确率在55%左右。 基础知识 树形结构属于非线性结构、层次结构，可以顺序存储或者链式存储，操作有遍历和查找。属于“一对多”的数据类型。 森林指m棵互不相交的树的集合。 二叉树是有序的，特殊的二叉树： 满二叉树 完全二叉树 叶子结点只能出现在最下层和次下层，且最下层的叶子结点集中在树的左部。 哈夫曼树(最优二叉树) 二叉树中的关键概念： 度 即结点的子树个数 度为0的结点为叶子结点 度为1的结点有1个分支 度为2的结点有2个分支 二叉树中不存在度大于2的结点 总结点数总比总度数多1 二叉树的度数与结点数 假设总度数：k，叶子结点、度为1和度为2的结点个数分别即为$n_0,n_1,n_2$ 度：$k = n_2 \times 2 + n_1$ 结点：$k+1 = n_2 + n_1 + n_0$ 综合二式：$n_0 = n_2 + 1$ 二叉树的高度与结点数 高度为h的二叉树，每层最多有$2^{h-1}$个结点。 对于高度为h满二叉树，其总结点数$2^h-1$。 具有N个结点的完全二叉树的高度h为$(\log_2 N)+1$ 二叉树的遍历 遍历 顺序 先序 根→左子树→右子树 中序 左子树→根→右子树 后序 左子树→右子树→根 层次 从上到下，从左到右 遍历经常出的选择题型： 已知两种遍历顺序，例如中序和后序，要求确定一棵二叉树。 首先后序可以确定根结点，结合中序，区分左子树和右子树。再分别针对左子树和右子树，集合中序和后序，慢慢还原一棵二叉树。 编程的题目还没有得到有效训练，LeetCode由于对树有内部编码，与本地自己实现的编程有差异。 哈夫曼树(最优二叉树) 带权路径最短的二叉树。$$WPL=\sum_{i=1}^{n} {W_i}{I_i}$$其中，二叉树共有n个叶子结点，$W_i$:第i个叶子结点的权，$I_i$:根到第i个叶子结点的路径长度。 哈夫曼编码 需要编码的字符集合为{$C_1,C_2,…,C_n$}，各个字符在电文出现的次数集合为{$w_1,w_2,…,w_n$}，用$C_1,C_2,…,C_n$作为叶子，$w_1,w_2,…,w_n$作为各叶子的权构造一棵哈夫曼树。 哈夫曼树中，左分支上标0，右分支上标1，$C_i$的编码顺序为从根到该叶子结点$C_i$的顺序。 几种转换 树转换为二叉树 由于普通的树是无序的，将其转换为二叉树的步骤： 加线：所有兄弟结点之间加一条线 抹线：对树中的每一个结点，只保留它与第1个孩子结点之间的连线，删除它与其它孩子结点的连线 旋转：把虚线改为实线从水平方向向下旋转45°形成二叉树。 森林转换为二叉树 森林由若干棵树组成。 先把每棵树转换为二叉树 第一棵二叉树不动，从第二棵二叉树开始，依次把后一棵二叉树的根结点作为前一棵二叉树的根结点的右孩子结点，用线连接起来。当所有的二叉树连起来后得到的二叉树就是由森林转换得到的。 栈 日期：2018/07/03 23:00 基础知识 栈和队列都是特殊的线性表，只能有部分操作的线性表。 特点：插入和删除操作都在线性表的一端(栈顶)进行，即按“后进先出”的规则进行操作。 top指针永远指向栈顶： 空栈：top=-1 满栈：top=n(n为数组大小) 每压入1个元素进栈，则指针+1 存储 顺序栈 必须预先分配固定大小内存空间。 静态分配内存容易出现浪费空间或者上溢的情况。 链栈 动态结点分配消除存储空间上的限制，避免“栈上溢”错误。 出栈顺序 入栈顺序:ABCDEF，如果没有特别指明，可能指示： A入栈，A出栈，B入栈，B出栈等 前缀表达式 前缀表达式的计算机求值特点： 从右至左扫描表达式 遇到数字时，将数字压入堆栈 遇到运算符时，弹出栈顶的两个数，用运算符对它们做相应的计算，并将结果入栈 重复上述过程直至表达式最左端 最后运算得出的值即为表达式的结果(中缀结果) 汉诺塔(Hanoi) 涉及递归与栈的知识。 题目：4个圆盘的Hanoi塔,总的移动次数为多少？ 设f(n)为n个圆盘的hanoi塔总的移动次数， 其递推方程为f(n)=f(n-1)+1+ f(n-1)=2*f(n-1)+1。 理解就是先把上面n-1个圆盘移到第二个柱子上(共f(n-1)步）， 再把最后一个圆盘移到第三个柱子(共1步）， 再把第二柱子上的圆盘移动到第三个柱子上（共f(n-1)步）。 而f（1）=1；于是f(2)=3,f(3)=7,f(4)=15。 进一步，根据递推方程其实可以得出f(n) = 2^n - 1。 复杂度 时间复杂度 用栈顶指针表示栈顶，栈的插入和删除操作均在栈顶进行。 因此对于顺序存储和链式存储的栈，进行插入和删除运算的时间复杂度均为O(1)。 空间复杂度 卡特兰数 题目：若一序列的进栈顺序为A,B,C,D,E,问存在多少种可能的出栈序列？ $$f(n)=\frac{C_{2n}^n}{n+1}$$ 如上题，答案应为42。 哈希 日期：2018/08/06 16:04 基本知识 哈希表就是一种以 键-值(key-indexed) 存储数据的结构，输入待查找的值即key，即可查找到其对应的值。 像python中的字典就是基于哈希表实现的。 使用哈希函数将被查找的键转换为数组的索引。在理想的情况下，不同的键会被转换为不同的索引值，但是在有些情况下我们需要处理多个键被哈希到同一个索引值的情况。所以哈希查找的第二个步骤就是处理冲突 处理哈希碰撞冲突。 有很多处理哈希碰撞冲突的方法，本文后面会介绍拉链法和线性探测法。 哈希表是一个在时间和空间上做出权衡的经典例子。 如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)； 如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。 哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 哈希函数 哈希查找第一步就是使用哈希函数将键映射成索引。这种映射函数就是哈希函数。 哈希函数需要易于计算并且能够均匀分布所有键。 比如举个简单的例子，使用手机号码后三位就比前三位作为key更好，因为前三位手机号码的重复率很高。再比如使用身份证号码出生年月位数要比使用前几位数要更好。 在实际中，我们的键并不都是数字，有可能是字符串，还有可能是几个值的组合等，所以我们需要实现自己的哈希函数。 直接定址法 直接定址法的优点很明显，就是它不会产生重复的hash值。但由于它与键值本身有关系，所以当键值分布很散的时候，会浪费大量的存储空间。所以一般是不会用到直接定址法的。 1hash(k)= a*k+b 避免哈希冲突 定义一个hash函数hash(k)=k mod 10，假设key:[15,1,24,32,55,64,42,93,82,76] 0 1 2 3 4 5 6 7 8 9 1 32 93 24 15 76 42 64 55 82 开放定址法 $$hash_{i}=(hash(key)+d_{i}) mod m, i=1,2,…,k(k \leq m-1)$$ 其中，hash(key)是哈希函数，di是增量序列，i为已冲突的次数。 线性探测法 $$d_{i}=1,2,…,(m-1)$$ 即$d_{i}=i$，或者其它线性函数。相当于逐个探测存放地址的表，直到查找到一个空单元，然后放置在该单元。 对于[15,1,24,32,55,64,42,93,82,76]: 可以看到，在55之前都还没冲突： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| | | 1 | 32 | | 24| 15| | | | | 此时插入55，与15冲突，应用线性探测，此时i=1，可以得到： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| | | 1 | 32 | | 24| 15|55 | | | | 再插入64，冲突不少，要取到i=3： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| | | 1 | 32 | | 24| 15|55 | 64| | | 插入42，i=1： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| | | 1 | 32 | 42| 24| 15|55 | 64| | | 插入93，i=5： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| | | 1 | 32 | 42| 24| 15|55 | 64| 93| | 插入82，i=7： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| | | 1 | 32 | 42| 24| 15|55 | 64| 93| 82| 插入76，i=4： | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | |—|---|—|---|—|---|—|---|—|---| |76 | 1 | 32 | 42| 24| 15|55 | 64| 93| 82| 发现越到后面，冲突的越来越离谱。所以，表的大小选择也很重要，此例中选择了10作为表的大小，所以容易产生冲突。一般来讲，越是质数，mod取余就越可能分布的均匀。 平方探测 伪随机探测 链表法 拉链法解决冲突时，需要使用指针，指示下一个元素的存储位置。 各链表上的结点空间是动态申请的,故链表法更适合于造表前无法确定表长的情况。 拉链法的缺点是：指针需要额外的空间，故当结点规模较小时，开放定址法较为节省空间 数据结构总结 Hash操作能根据散列值直接定位数据的存储地址，设计良好的hash表能在常数级时间下找到需要的数据，但是更适合于内存中的查找。 B+树是一种是一种树状的数据结构，适合做索引，对磁盘数据来说，索引查找是比较高效的 STL_Map的内部实现是一颗红黑树，但是只是一颗在内存中建立二叉树树，不能用于磁盘操作，而其内存查找性能也比不上Hash查找。 算法 排序 日期：2018/07/04 21:31 分类 插入排序 不到最后一轮排序，存在完全变换序列位置的可能性。 直接插入排序 稳定排序方式。 折半插入排序(二分插入排序) 不太理解，也是稳定的排序方式。 希尔排序(缩小增量排序) 不稳定的排序方式。 交换排序 冒泡排序 冒泡排序最多要交换n(n-1)/2次。 两两比较和交换，是一种稳定的排序方式。 快速排序(划分排序) 以序列第一个元素为基准，两端各一个指针。 速度最快，但序列有序的情况下，反而插入排序更快。 但是一种不稳定的排序方式。 选择排序 简单选择排序 每一趟选出最小的，是一种不稳定的排序方式。 - 树形选择排序 从叶子节点开始，兄弟节点之间两两比赛，胜者上升到父节点。 是一种不稳定的排序方式。 堆排序 大根堆：从上到下-从大到小 小根堆：从上到下-从小到大 是一种不稳定的排序方式。 归并排序 最常见的是二路归并排序。 是一种要求内存最大、稳定的排序方式。 基数排序 多关键字排序 是一种稳定的排序方式。 链式基数排序 是一种稳定的排序方式。 外部排序 基本方法 多路归并排序 置换-选择排序 归纳表 图的遍历算法 深度优先搜索(Deep First Search,DFS) 对连通图进行遍历的算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Gragh(): def __init__(self,nodes,sides): ''' nodes 表示点 sides 表示边 ''' # self.sequense是字典，key是点，value是与key相连接的点 self.sequense = &#123;&#125; # self.side是临时变量，主要用于保存与指定点相连接的点 self.side=[] for node in nodes: for side in sides: u,v=side # 指定点与另一个点在同一个边中，则说明这个点与指定点是相连接的点，则需要将这个点放到self.side中 if node ==u: self.side.append(v) elif node == v: self.side.append(u) self.sequense[node] = self.side self.side=[] #print self.sequense ''' # Depth-First-Search 深度优先算法，是一种用于遍历或搜索树或图的算法。沿着树的深度遍历树的节点，尽可能深的搜索树的分支。 当节点v的所在边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。 这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发现的节点， 则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止。属于盲目搜索。 ''' def DFS(self,node0): #queue本质上是堆栈，用来存放需要进行遍历的数据 #order里面存放的是具体的访问路径 queue,order=[],[] #首先将初始遍历的节点放到queue中，表示将要从这个点开始遍历 queue.append(node0) while queue: #从queue中pop出点v，然后从v点开始遍历了，所以可以将这个点pop出，然后将其放入order中 #这里才是最有用的地方，pop（）表示弹出栈顶，由于下面的for循环不断的访问子节点，并将子节点压入堆栈， #也就保证了每次的栈顶弹出的顺序是下面的节点 v = queue.pop() order.append(v) #这里开始遍历v的子节点 for w in self.sequense[v]: #w既不属于queue也不属于order，意味着这个点没被访问过，所以讲起放到queue中，然后后续进行访问 if w not in order and w not in queue: queue.append(w) return order 广度优先搜索(Breadth First Search,BFS)]]></content>
      <categories>
        <category>IT应聘</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随记-论计划的重要性]]></title>
    <url>%2F2018%2F05%2F30%2F%E9%9A%8F%E8%AE%B0-%E8%AE%BA%E8%AE%A1%E5%88%92%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%2F</url>
    <content type="text"><![CDATA[20180902更新： 收获 果然，痛定思痛反省，努力争取还是有机会的，前2日刚得知论文录取了，而且没有什么实质性的审稿意见😃不出意外的话3个月内被EI检索。7月份没白熬夜啊，终于苦尽甘来，收到Acceptence邮件时的那种喜悦，那种释然，矫情了😄终于可以毕业啦! 研究生履历： 2016年9月-2017年6月：研一在校上课 2017年7月-2017年12月：Splunk实习 2018年1月-2018年5月：完成第一篇小论文 2018年6月-2018年7月：中文核心期刊被拒，英文EI会议录取 2018年8月-至今：找工作，投简历，笔试 计划赶不上变化，6月投递中文核心期刊《计算机应用》被拒，7月投递ICSAI会议，8月才开始找工作投简历，一边参加校招笔试，一边复习笔试，根本没时间提前准备😣只能硬着头皮上了！恍恍惚惚也到了九月校招季了，加油👊 原文： 回首 研究生生涯已过大半，学生生活也无多时日了，即将到来的招聘季，也预示着新生活的到来。 今天发出这感慨，全是由于昨日师姐的谢师宴，得知我错过了发SCI副刊的机会。期刊给老师的约稿，其实就是给老师一个版面，我的论文完全是有机会的，然而我却这样失之交臂。 总结下来，原因有二： 1.自信息缺乏，从老师发给我约稿文件，看明白是SCI之后，我就打心里觉得我不行，我肯定够不上SCI的门槛😔所以前期也没有积极问老师相关信息，直至昨日氛围轻松的饭局上，才得知这是比自己发中文核心还容易的事。 2.主次不分，一直有这个毛病，分不清孰重孰轻，眼光局限，明明努力一下可以发SCI的却把宝贵的大半个月时间浪费在其他事情上。 总是看着别人取得的优秀成绩，羡慕别人又讨伐自己，看似每日用功读书，实则浑浑噩噩，没有目标，没有计划。不知到毕业最后谢幕，我的简历上会呈现何番模样？我的研究生生活丰富与否？三年前的选择对不对？😕😕😕 展望 接下来到８月底找工作，我还有满打满算３个月时间来准备扩充的我的知识库，准备招聘。 2018年6月-2018年8月计划： 1.将第一篇小论文投中文核心 2.写第二篇小论文投校报 3.准备招聘笔试题目 4.数据分析技能综合培养 5.开发完整的数据分析项目]]></content>
      <categories>
        <category>生活随记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git问题备忘]]></title>
    <url>%2F2018%2F05%2F29%2FGit%E9%97%AE%E9%A2%98%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[更新本地库 在LeetCode上，每做完一道题目，为方便后续回顾，同步远程Github。 123git add 需要同步的文件git commit -m &quot;备注&quot;git push -u origin master 远程仓库与本地仓库冲突 由于在远程仓库创建过新文件，导致与本地仓库版本冲突无法合并： 12345678$ git push origin masterTo github.com:sunflowerJY/LeetCode.git ! [rejected] master -&gt; master (non-fast-forward)error: failed to push some refs to &apos;git@github.com:sunflowerJY/LeetCode.git&apos;hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: &apos;git pull ...&apos;) before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 解决方案： http://www.cnblogs.com/daemon369/p/3204646.html]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化Next主题博客]]></title>
    <url>%2F2018%2F05%2F15%2F%E4%BC%98%E5%8C%96Next%E4%B8%BB%E9%A2%98%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[在上篇博文中已经搭建好一个基础的可用的博客了，但是热爱折腾的我们怎么可能就此满足😎此文将介绍我的博客折腾之路~ 环境 系统：Win10 64位 Git: 2.17.0.windows.1 Node.js: 8.11.1 npm: 5.6.0 Hexo: 3.7.1 Next: 5.1.4 Next主题 选择一个自己可心的主题，会增强更博的动力哟😜 下载Next主题 在Git bash终端执行以下命令： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 本文使用的是Next: 5.1.4，目前已经更新至6.0,由于自己还是新手，基本搭建过程都是参考大神博客的，所以以后熟练了再更新吧😂 下载成功后可以在本地博客站点的主题文件夹/themes中除了默认的landscape文件夹，还多了一个next文件夹。 使用Hexo搭建博客时，由于在多个文件夹中存放了_config.yml配置文件，为表述方便，规定： 1.站点配置文件：sunflowerJY.github.io/_config.yml 2.主题配置文件：sunflowerJY.github.io/themes/next/_config.yml 启用Next主题 打开站点配置文件，找到theme字段，并将其值更改为 next。 1theme: next 预览Next风格的本地博客 在Git bash终端执行： 1hexo s 不知为何，我的hexo命令在站点文件夹以外的目录使用会报错找不到该命令，故我都会在站点根目录执行上述命令。 在浏览器中访问 http://localhost:4000/ 便可预览本地博客效果。❤️ 修改Next主题的Scheme Next主题的默认Scheme是Muse,效果如先上图。本博客使用的是Pisces，编辑主题配置文件中的Scheme字段修改Next主题的风格： 1scheme: Pisces 修改网站的基本信息 修改站点配置文件： 12345678# Sitetitle: 向日葵的自留地subtitle:description:keywords:author: sunflowerJYlanguage: zh-hans #汉语timezone: 修改微博底部添加访客数 themes/next/layout/_partials/footer.swig删除多余信息，保留如下信息： 12345678910111213141516171819202122&lt;div class=&quot;copyright&quot;&gt;&#123;##&#125;&#123;% set current = date(Date.now(), &quot;YYYY&quot;) %&#125;&#123;##&#125;&amp;copy; &#123;% if theme.footer.since and theme.footer.since != current %&#125;&#123;&#123; theme.footer.since &#125;&#125; &amp;mdash; &#123;% endif %&#125;&#123;##&#125;&lt;span itemprop=&quot;copyrightYear&quot;&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt; &lt;span class=&quot;with-love&quot;&gt; &lt;i class=&quot;fa fa-&#123;&#123; theme.footer.icon &#125;&#125;&quot;&gt;&lt;/i&gt; &lt;/span&gt; &lt;span class=&quot;author&quot; itemprop=&quot;copyrightHolder&quot;&gt;&#123;&#123; theme.footer.copyright || config.author &#125;&#125;&lt;/span&gt; &#123;% if theme.post_wordcount.totalcount %&#125; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;post-meta-item-icon&quot;&gt; &lt;i class=&quot;fa fa-area-chart&quot;&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.post_wordcount.item_text %&#125; &lt;span class=&quot;post-meta-item-text&quot;&gt;&#123;&#123; __(&apos;post.totalcount&apos;) &#125;&#125;&amp;#58;&lt;/span&gt; &#123;% endif %&#125; &lt;span title=&quot;&#123;&#123; __(&apos;post.totalcount&apos;) &#125;&#125;&quot;&gt;&#123;# #&#125;&#123;&#123; totalcount(site, &apos;0,0.0a&apos;) &#125;&#125;&#123;# #&#125;&lt;/span&gt; &#123;% endif %&#125;&lt;/div&gt; 在上面copyright代码前添加： 1&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 在最后添加： 12345&lt;div class=&quot;powered-by&quot;&gt;&lt;i class=&quot;fa fa-user-md&quot;&gt;&lt;/i&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站访客数:&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; 简化后的微博底部效果如下： 添加头像 修改主题配置文件中的avatar,值设置为头像的链接： 1234# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gif（新建 uploads 目录若不存在）avatar: /images/avatar.jpg 配置完成部署更新后便可以在HOME页看到自己的头像啦😃 设置网站图标 将图片在ico图片生成网站上分别生成16x16和32x32的图表，并改名为favicon-16x16-sunflower.ico和favicon-32x32-sunflower.ico，并放到/themes/next/source/images中，修改主题配置文件的small和medium字段： 1234567favicon: small: /images/favicon-16x16-sunflower.ico medium: /images/favicon-32x32-sunflower.ico apple_touch_icon: /images/apple-touch-icon-next.png safari_pinned_tab: /images/logo.svg #android_manifest: /images/manifest.json #ms_browserconfig: /images/browserconfig.xml 效果如图： 添加动态背景 Next主题版本》=5.1.1可以直接在主题配置文件修改： 12# Canvas-nestcanvas_nest: true 添加顶部加载条 修改主题配置文件将pace: false改为pace: true，还可以换不同风格的加载条： 添加分类和标签 分类(Categories)和标签(tags)能够很好得将博客文章进行分类管理，方便查阅归类。 添加分类 在博客文件夹下，执行： 1hexo new page categories 自动在/source/categories/文件下创建index.md文件: 1234 ---title: 分类date: 2018-05-15 15:39:08--- 如果有集成评论服务，分类页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，修改后的index.md： 123456---title: 分类date: 2018-05-15 15:39:08type: &quot;categories&quot;comments: false--- 修改菜单： 在菜单中添加链接。编辑主题配置文件， 添加categories到menu中，如下: 1234menu: home: / archives: /archives categories: /categories 添加标签 在博客文件夹下，执行： 1hexo new page tags 自动在/source/tags/文件下创建index.md文件: 1234 ---title: 标签date: 2018-05-15 15:39:08--- 如果有集成评论服务，标签页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，修改后的index.md： 123456---title: 标签date: 2018-05-15 15:39:08type: &quot;tags&quot;comments: false--- 修改菜单： 在菜单中添加链接。编辑主题配置文件， 添加tags到menu中，如下: 1234menu: home: / archives: /archives categories: /tags 修改标签图标 Next主题中设置了标签后，默认在文章最底部显示#，后面接文章所有的标签。总是觉得#这个标识丑丑的😂😂😂还是改一下吧 修改/themes/next/layout/_macro/post.swig：搜索rel=&quot;tag&quot;&gt;#,将#换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;，效果图： 添加本地搜索 安装hexo-generator-searchdb，在站点的根目录下执行以下命令： 1npm install hexo-generator-search --save 编辑站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件，启用本地搜索功能： 1234local_search: enable: true trigger: auto top_n_per_article: 1 修改首页摘要 修改主题配置文件： 123auto_excerpt: enable: true length: 150 修改文内链接式样 在/themes/next/source/css/_common/components/post/post.styl文末添加： 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #FF6600; border-bottom: none; border-bottom: 1px solid #FF6600; &amp;:hover &#123; color: #FF6600; border-bottom: none; border-bottom: 1px solid #FF6600; &#125;&#125; 其中，.post-body是为了不影响标题，选择p是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义，效果如图： 添加emoji表情包 把原来的hexo渲染换成基于markdown-it的渲染: 12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --save 在站点根目录node_modules\hexo-renderer-markdown-it\下安装markdown-it-emoji插件： 1npm install markdown-it-emoji --save 编辑站点配置文件，添加如下内容： 12345678910111213141516171819202122# markdown-it配置emoji表情包markdown: render: html: true xhtmlOut: false breaks: true linkify: true typographer: true quotes: &apos;“”‘’&apos; plugins: - markdown-it-abbr - markdown-it-footnote - markdown-it-ins - markdown-it-sub - markdown-it-sup - markdown-it-emoji #用emoji插件 anchors: level: 1 collisionSuffix: &apos;v&apos; permalink: true permalinkClass: header-anchor permalinkSymbol: &apos;&apos; 然后就可以在写博的时候加上emoji表情啦😄 添加Gitment评论系统 注册OAuth Application 登陆Github官网，进入https://github.com/settings/developers: 点击绿色Register a new application，填写以下内容： 点击绿色Register application,可获得Client ID、Client Secret。 创建Github仓库 创建一个仓库存放评论，仓库名：gitment-comments 修改主题配置文件 12345678910111213141516# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/# You can get your Github ID from https://api.github.com/users/&lt;Github username&gt;gitment: enable: true mint: true count: true lazy: false cleanly: false language: github_user: sunflowerJY #Github用户名 github_repo: gitment-comments #仓库名 client_id: 5321cd82f4205f8b784e client_secret: 50caad7a61fb5f6d84fc9a66653e6ff625d1db57 proxy_gateway: redirect_protocol: 上述信息中除了github_user、github_repo、client_id、client_secret需要注意外，其他信息均可默认不改。 部署到博客查看效果： 1hexo d 初始化本文的评论页 使用评论功能要登陆Github账号，登陆后点击初始化本文的评论页。 💔由于文章标题过长的原因，可能会报错Error：validation failed: 修改/themes/next/layout/_third-party/comments/gitment.swig文件中标亮的代码： 用文章的时间替代标题标识评论，避免了标题超出50个字符的问题。 管理评论 在gitment-comments的Issues中可以集中管理博客所有文章的评论。 添加萌宠 给孤独写博人来个伴儿吧😬这里有各种各样的宠物,pick一个心水的萌宠吧😉 安装模块： 1npm install --save hexo-helper-live2d 下载一个宠物包, 以我博客中的黑猫为例： 1npm install live2d-widget-model-hijiki 站点配置文件中添加以下字段： 1234567891011121314151617# live2d live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ model: use: live2d-widget-model-hijiki/ display: position: left width: 80 height: 300 hOffset: 20 #水平相对位移 vOffset: -200 #垂直相对位移 mobile: show: true 其中，注意model.use填写自己下载的宠物包名字。 Hexo-admin一站式写博部署 写第一篇博文的时候，总是纠结困惑排版的问题，网上也有很多推荐的Markdown编辑器，但是由于种种原因，和部署到Github Pages还是会有很多本地没有出现的问题。直到偶然之间，发现Hexo-admin这个插件，可以直接在本地http://localhost:4000/admin/ 写博客，并实时查看排版效果，比如我正在书写的这篇博文： 点击红圈的位置，可以实现实时本地预览： 一键式部署到Github Pages,还可以顺带部署信息，方便以后版本查阅: 是不是和Github Pages用起来无缝连接😉以下实现步骤： 安装插件： 1npm install --save hexo-admin 本地运行,打开http://localhost:4000/admin/ ： 1hexo server -d 💔上述的Deploy可能第一次使用会报错，解决方案如下： 编辑站点配置文件，搜索admin字段修改如下： 123# hexo-admin admin.deployCommandadmin: deployCommand: &apos;sh hexo-deploy.sh&apos; 在站点根目录下新建hexo-deploy.sh文件，内容如下： 1hexo g -d 编辑站点根目录下node_modules/hexo-admin/deploy.js文件，修改标亮处： 1var proc = spawn((process.platform === &quot;win32&quot; ? &quot;hexo.cmd&quot; : &quot;hexo&quot;), [&apos;d&apos;, &apos;-g&apos;]); 安装图床插件 目前上传图片至千牛网生成外链，复制到博客的html语言生成图片，知道有hexo-qiniu-sync这么一个插件能够化繁为简，去除重复劳动，使得在博客中添加图片更方便。但是一直安装配置存在问题，导致该插件无法使用，并且导致基本的hexo-clean、hexo s等命令也报错。 很无奈，未完待续。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Github Pages</tag>
        <tag>Hexo</tag>
        <tag>Next</tag>
        <tag>emoji</tag>
        <tag>Gitment</tag>
        <tag>Hexo-admin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github Pages + Hexo搭建博客填坑实记]]></title>
    <url>%2F2018%2F05%2F13%2FGithub%20Pages%20%2B%20Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%A1%AB%E5%9D%91%E5%AE%9E%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[背景 从实习结束，开始正儿八经看论文的时候，开始萌生了写博的想法。研一寒假在家不想看论文刷LeetCode的时候，学习了很多大神优秀的解法，却奈何下次遇到类似的方法还是困惑，便想找个地儿记录下来。 一开始选择了博客网站——类似博客园或者CSDN之类的，图形界面，操作简便，但存在广告，排版杂乱的问题。无疑之间发现了Markdown——写博利器，简洁明了，兼容Jupyter Notebook，同时显示代码和文字说明，非常适合数据分析工作。 这是我的博客成品：https://sunflowerjy.github.io/ 搭建此博客步骤很简单，顺利的话最快半小时就能搭建成功。然而标题是填坑实记，一看就是有故事的人😑故事听我娓娓道来~ 环境 系统：Win10 64位 Git: 2.17.0.windows.1 Node.js: 8.11.1 npm: 5.6.0 Hexo: 3.7.1 30分钟搭建最基础博客 写在最前面，提示：此步骤只适合环境与我一样，因为之前我也是参考别人分享的博文，然而由于各种软件的更新导致很多步骤都会报错，所以后期有可能由于软件更新而导致按照本文的方法安装配置出现错误。 安装Git Git的下载地址(Windows版本)：https://git-scm.com/download/win 选择适合自己系统的版本。 除了以下两个步骤以外，都可以默认选择Next(下一步): 如何使用Git? 标注 1：仅使用 Git Bash 进行操作； 标注 2：在选择使用 Git Bash 进行操作的同时，也可以使用 Windows 命令行操作，建议选择此项； 标注 3：在选择使用 Git 的同时，也把 Unix 工具加入到了我们的配置之中，而且此操作会覆盖 Windows 的一些工具，强烈不建议选择此项。 关于回车换行的问题 Dos和Windows采用回车+换行CRLF表示下一行，而Unix/Linux采用换行符LF表示下一行。 第一个选项是默认检查Windows风格的文件，并在提交时转换为Unix风格。 请选择第三项。如果默认选择第一项，在git中提交代码可能会报错。不过可以安装好之后再更改相关配置。若后续commit报错请参考搭建博客遇到的各种坑的详细介绍。 检验Git是否安装成功： 安装成功后，可以在桌面鼠标右键找到Git bash： 安装Node.js Node.js下载地址：https://nodejs.org/en/download/ 选择自己对应系统的版本。一路默认Next安装。 新版本的Node.js中集成了npm工具，故不需要再另外安装。 检验Node.js和npm是否安装成功： 安装Hexo 1npm install hexo-cli -g 检验Hexo是否安装成功： 本地搭建博客 在本地计算机新建一个文件夹存放博客代码： 1hexo init sunflowerJY.github.io 我存放博客代码的文件夹名：sunflowerJY.github.io 生成静态文件： 1hexo g # 或者hexo generate 在sunflowerJY.github.io文件夹下生成public文件夹存放静态文件。 启动本地Web服务： 1hexo s # 或者hexo server 现在可以在本地查看自己搭建的博客啦😃http://localhost:4000 关联Github 注册Github账号 Github官网：https://github.com/ 注册过程跟大部分网站相似，尤其注意注册邮箱user.email和用户名user.name,后期部署博客要用到并且必须一字不差，这也是遇到的坑之一😂 用户名： 创建Github仓库 创建Github仓库来远程保存备份本地的博客代码。 这里再强调下，仓库名必须命名为用户名.github.io，之前由于我没有严格遵守用户名这一点，导致部署到Github的博客css等文件无法识别😩折腾了好久还是别人帮我检查到的😂 由于我的博客仓库已经搭建过所以提示报错已存在该仓库，新创建的不会报错。 开启Github Pages功能 打开博客仓库的Setting页面： 拉至Github Pages选项，将Source选择master分支： 可能会提示仓库为空，要先添加文件，请将本地的博客push到该远程仓库。熟悉Git命令的请跳过，否则参考上传本地博客仓库至远程仓库 配置Git账号信息 设置git的全局账号信息，user.name为Github用户名，user.email为Github注册邮箱。 12git config --global user.name &quot;sunflowerJY&quot;git config --global user.email &quot;sunflower_jy@icloud.com&quot; 查看Git的所配置信息： 1git config --list 配置SSH 查看本地的SSH Key，桌面右键进入Git bash： 1ls -al ~/.ssh 若有id_rsa和id_rsa.pub两个文件，则复制id_rsa.pub中内容至Github设置中。 New SSH Key: 将id_rsa.pub中的内容复制到Key中： 验证SSH是否配置成功： 1ssh -T git@github.com 成功的话会显示以下的大致内容： 123456The authenticity of host &apos;github.com (192.30.252.128)&apos; can&apos;t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;github.com,192.30.252.128&apos; (RSA) to the list of known hosts.Hi sunflowerJY! You&apos;ve successfully authenticated, but GitHub does not provide shell access.Connection to github.com closed. 上传本地博客仓库至远程仓库 在Git bash中cd到本地博客文件夹sunflowerJY.github.io： 123git initgit add .git commit -m &quot;first commit blog&quot; 修改配置文件部署部分 修改本地博客文件夹sunflowerJY.github.io中的_config.yml： 其中，repo填写自己的仓库地址，主要修改对应的用户名位置。 安装Deploy工具 1npm install hexo-deployer-git --save 部署至Github 1hexo d#或hexo deploy 至此已经成功搭建了自己的博客，快去查看自己的专属博客吧💕https://用户名.github.io 搭建博客遇到的各种坑 其实在上一节30分钟搭建最基础博客中基本都已经提及了，现在来总结下吧 关于回车换行的问题 上一节在安装Git中提到过，由于Windows和Linux的换行规则不一样，所以为了确保在将Windows平台上的文件commit至Github时能正常使用，Git给出了自动转换文件格式的服务，即下图中的第一个选项。若默认选择了这项Next,后序在使用Git的过程中可能会报crlf与lf文件转换的WARN。 解决方案： 如果你是Windows程序员，且正在开发仅运行在Windows上的项目，可以设置false取消此功能，把回车符记录在库中： 1git config --global core.autocrlf false Github仓库名 Github仓库名必须以用户名.github.io的形式命名，用户名那里要一字不差，不然后期的css等文件渲染会有问题，一开始我没注意到这个问题导致博客打开傻眼了，白花花的一片只有字😱💥😢 开启Github Pages功能 如果仓库内没有文件，页面上会提示无法开启Github Pages功能，可以参照上传本地博客仓库至远程仓库。随即仓库中有文件后，便可开启Github Pages功能。 搭建博客遇到的各种坑这块内容我会一直保持更新。随时记录，方便有需要的人，也方便自己备忘😂 关于博客的内容后续还会更新，例如打造个性化的博客,如有需要请关注教程标签。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Github Pages</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
